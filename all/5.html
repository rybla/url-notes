<!DOCTYPE html><!--lyanoQfPS9ZzI0wL9d9zm--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/url-notes/_next/static/css/f03c873af434c7c6.css" data-precedence="next"/><link rel="stylesheet" href="/url-notes/_next/static/css/0e5ea1ea0183b412.css" data-precedence="next"/><link rel="stylesheet" href="/url-notes/_next/static/css/7190d9c623ab1fe0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/url-notes/_next/static/chunks/webpack-54d9d4b2aa649176.js"/><script src="/url-notes/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/url-notes/_next/static/chunks/964-a29425d4972030f1.js" async=""></script><script src="/url-notes/_next/static/chunks/main-app-e4d4697bcd6cfe75.js" async=""></script><script src="/url-notes/_next/static/chunks/874-437a265a67d6cfee.js" async=""></script><script src="/url-notes/_next/static/chunks/app/all/%5BpageIndex%5D/page-680db69597409d20.js" async=""></script><title>url-notes | all | page 6 of 20</title><link rel="icon" href="/url-notes/favicon.ico" type="image/x-icon" sizes="256x256"/><script src="/url-notes/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><div class="page_Page__nS49m"><div class="Header_Header__VDN4T"><div class="Header_item__PWJos"><a href="/url-notes">url-notes</a></div><div class="Header_separator__zF22U">|</div><div class="Header_item__PWJos"><a href="/url-notes/all/0">all</a></div><div class="Header_separator__zF22U">|</div><div class="Header_item__PWJos"><span>page <!-- -->6<!-- --> of <!-- -->20</span></div></div><div class="page_content___i7Ut"><div class="page_previews__BuBMS"><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2506.17871">How Alignment Shrinks the Generative Horizon</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag"></a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Computation%20and%20Language">Computation and Language</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Artificial%20Intelligence">Artificial Intelligence</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Machine%20Learning">Machine Learning</a></div><div class="ArticlePreview_summary__Zyb4E"><p>Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model&#x27;s output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model&#x27;s output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model&#x27;s behavior, but instead steers it toward stylistic tokens (e.g., &quot;Sure&quot;) that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.</p></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2506.21521">Potemkin Understanding in Large Language Models</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Computation%20and%20Language">Computation and Language</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Artificial%20Intelligence">Artificial Intelligence</a></div><div class="ArticlePreview_summary__Zyb4E"><p>Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM&#x27;s capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.</p></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://download.ssrn.com/2025/6/30/4440242.pdf?response-content-disposition=attachment%3B%20filename%3Dssrn-4440242.pdf&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjENb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDmvD0kwhxUvwfI51pkPe7LHJvlbunkSaSJ%2F%2FWUHoHLxwIgKvlIsW8iuAAP7s8oth%2F9YaKIKpRZ81hCBzCyI8YDF5oqxQUIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzMDg0NzUzMDEyNTciDPFDIhL%2BP88i2OXZ6SqZBefQZOwOYYLdg%2BqExabceAZ%2BrERHWnxBuYo%2BE3mBpdQ5F230f6aZ0YPQPQLAlDLggStpWurCRPjzdUpiah95RWY5%2Fho%2B%2BIJMQt%2BXlP2zJ0tH%2Bjc8L44sVb5xro0WdeoEbn9kv1HEEgPlyJYK9VnuSWakRRvq2rYBQF9GGYtOSNRKqubOHX3sn0Otqt0i4xN5FuayIg0CPeZtJ2ZgN%2FEH%2FDw6oir%2BvSvn%2FtOXrpYNe74yps2euHd9iiv21zZRIGlGwmKnrd%2F5lsjBUoLnOSd9VAa%2BqKrYp9ivQfcCB7Oqe9Rj4iLjh8TeVCyRqkXwBxbOOhxW8gomEJOcjOJpiTOeNooyzLgR7FZqo2h8ia6aVyhHQPTDvCQgtLLc%2FYQnd%2FQ9qZWwx2%2FbXqeXsNI5Or2qcgPy8Ul4%2F%2FeuLHbFsLA6FnY%2Fz5HbC%2BNoQP3SV3od4tYKPESNObQOMKixm7iq5TIdK1OzaeQhlSpGdFs7GWwiAWqXNDHVVFbdXLDczvSPpEaubfmUUhLiZaZA05HLMpMBrsDXgPfjVxwikIyC20x1NfE2G6diCN5zCaqR5l5Jh4lzKXTOKDZcs8iJs3I4mjCKmRn3BqSuWlaSI4K049EVQvifWBF8Oz42Yub0UXIbns659tipXWgnpe2t9dkVKEzRt4ENmyOi%2FaUHPxeQkigTmXHR0Cmb%2FYagZpfp5knBLBXlCjZxy3OlLDs8qZmvXreW4cBEts7UNy8IQLeuoxBYkr4nbNreSYRpTjw4tcG9zPmUUzuRVSt6VF07W5KJ1jQqTI4aJ4wtmWSgMFopPVGiyduaToM7%2FBxMY5gpV2ZsByt%2BJCObeZrUyRfq2HwHoWC5wGMiOjtsYe%2Bmetmp890mAOxL6VJowA%2Bl9Kh%2FMOnqjcMGOrEB18AnWgBDmlKkSscFIxJOL33ixrSai7YAjhHXpoVSUh3B0Z%2BMO2iO4XxEAzuzYhL9J%2BovhoVJ9GaFJdmMOqrxN1CRHitVibPr0mE0wZyNGqpu%2FjWOg6wvjLuET7Wxt7jxHMzmdpPEO26pBemUMHTSjbGtrC7Asc7wbAusIZ%2Fcuqy8B6MgLkiLi1QOIjBHzY7Vs6p2nu5u%2BjF4%2Fld1vyCVhsoH6J5EoMRRyixpRF4qv3T0&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20250701T054517Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAUPUUPRWE7AM5UVQA%2F20250701%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=50a47418d1bede25e5f619afb7a74d57c0d0600407a48e401744a96f24f59ed8&amp;abstractId=4440242">Cancel Culture</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/cancel%20culture">cancel culture</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/free%20speech">free speech</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/employee%20speech">employee speech</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/public%20discourse">public discourse</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/social%20media">social media</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/controversy">controversy</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/censorship">censorship</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/online%20shaming">online shaming</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/deplatforming">deplatforming</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/corporate%20policy">corporate policy</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/political%20polarization">political polarization</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/freedom%20of%20expression">freedom of expression</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/social%20justice">social justice</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/political%20correctness">political correctness</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/online%20activism">online activism</a></div><div class="ArticlePreview_summary__Zyb4E">&lt;div&gt;
 &lt;div&gt;
  Public controversies over employee speech where individuals face backlash for expressing views deemed controversial,Â &lt;span&gt;unpopular, or mis</div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://yosefk.com/blog/llms-arent-world-models.htmlhttps://arxiv.org/pdf/2505.17117">From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Computation%20and%20Language">Computation and Language</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Artificial%20Intelligence">Artificial Intelligence</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Information%20Theory">Information Theory</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/Information%20Theory">Information Theory</a></div><div class="ArticlePreview_summary__Zyb4E"><p>Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.</p></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://www.theregister.com/2025/03/29/malware_obscure_languages/">Malware in Lisp? Now you&#x27;re just being cruel</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/malware">malware</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/cybersecurity">cybersecurity</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/information%20security">information security</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/lisp">lisp</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/haskell">haskell</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/delphi">delphi</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/programming%20languages">programming languages</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/evasion%20techniques">evasion techniques</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/threat%20detection">threat detection</a></div><div class="ArticlePreview_summary__Zyb4E"><p>: Miscreants warming to Delphi, Haskell, and the like to evade detection</p></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://stackbench.ai/">Analyze how well coding agents use your libraries and frameworks</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ai">ai</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/llm">llm</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/coding%20agents">coding agents</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/software%20development">software development</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/library%20development">library development</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/frameworks">frameworks</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/api%20design">api design</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/developer%20experience">developer experience</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/dx">dx</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/documentation">documentation</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/testing">testing</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/agent-friendliness">agent-friendliness</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ai-friendliness">ai-friendliness</a></div><div class="ArticlePreview_summary__Zyb4E"><p>Test your library&#x27;s agent-friendliness by analyzing how well coding agents use your documentation</p></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://github.com/hasktorch/hasktorch">hasktorch/hasktorch</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/haskell">haskell</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/machine%20learning">machine learning</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/deep%20learning">deep learning</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/neural%20networks">neural networks</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/pytorch">pytorch</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/libtorch">libtorch</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/tensors">tensors</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/library">library</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ffi">ffi</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/c%2B%2B">c++</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/cuda">cuda</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpu">gpu</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/scientific%20computing">scientific computing</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/code%20generation">code generation</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/hasktorch">hasktorch</a></div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li><strong>Hasktorch:</strong> A Haskell library for tensors and neural networks.</li>
<li><strong>Core:</strong> It is a community project that wraps the C++ backend of PyTorch (libtorch).</li>
<li><strong>Status:</strong> The project is in active development (currently version 0.2) and the API is subject to change.</li>
<li><strong>Installation:</strong> Provides setup guides for multiple environments:<!-- -->
<ul>
<li><strong>OS:</strong> Linux, macOS, NixOS, Docker.</li>
<li><strong>Build Tools:</strong> <code>cabal</code> and <code>stack</code>.</li>
<li><strong>Hardware:</strong> CPU and NVIDIA CUDA.</li>
</ul>
</li>
<li><strong>Project Structure:</strong>
<ul>
<li><code>hasktorch/</code>: High-level, user-facing API.</li>
<li><code>libtorch-ffi/</code>: Low-level Foreign Function Interface (FFI) bindings to libtorch.</li>
<li><code>codegen/</code>: A code generation module that parses PyTorch&#x27;s <code>Declarations.yaml</code> to produce the FFI bindings.</li>
<li><code>examples/</code>: Contains example models and usage.</li>
</ul>
</li>
</ul></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://www.theverge.com/24167865/google-zero-search-crash-housefresh-ai-overviews-traffic-data-audience">Google Zero is here â now what?</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/technology">technology</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/internet">internet</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/web">web</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/search">search</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/search%20engines">search engines</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/google">google</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/platforms">platforms</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/web%20platforms">web platforms</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/future%20of%20technology">future of technology</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/technological%20change">technological change</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/platform%20shifts">platform shifts</a></div><div class="ArticlePreview_summary__Zyb4E"><p>Search is a foundational platform the modern web was built on. But all platforms come and go â so whatâs next?</p></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://dl.acm.org/doi/10.1007/11561347_18">Multi-stage programming with functors and monads | Proceedings of the 4th international conference on Generative Programming and Component Engineering</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/multi-stage%20programming">multi-stage programming</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/functional%20programming">functional programming</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ocaml">ocaml</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/metaocaml">metaocaml</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/monads">monads</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/functors">functors</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/generative%20programming">generative programming</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/metaprogramming">metaprogramming</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/compilers">compilers</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/type%20systems">type systems</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gaussian%20elimination">gaussian elimination</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/numerical%20algorithms">numerical algorithms</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/symbolic%20algorithms">symbolic algorithms</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/performance%20optimization">performance optimization</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/abstraction%20overhead">abstraction overhead</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/code%20generation">code generation</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/aspect-oriented%20programming">aspect-oriented programming</a></div><div class="ArticlePreview_summary__Zyb4E"><p>With Gaussian Elimination as a representative family of numerical and symbolic algorithms, we use multi-stage programming, monads and Ocaml&#x27;s advanced module system to demonstrate the complete elimination of the abstraction overhead while avoiding any inspection of the generated code. We parameterize our Gaussian Elimination code to a great extent (over domain, matrix representations, determinant tracking, pivoting policies, result types, etc) at no run-time cost. Because the resulting code is generated just right and not changed afterwards, we enjoy MetaOCaml&#x27;s guaranty that the generated code is well-typed. We further demonstrate that various abstraction parameters (aspects) can be made orthogonal and compositional, even in the presence of name-generation for temporaries and other bindings and âinterleavingâ of aspects. We also show how to encode some domain-specific knowledge so that âclearly wrongâ compositions can be statically rejected by the compiler when processing the generator rather than the generated code.</p></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://blog.danielh.cc/blog/passwords">We replaced passwords with something worse | Blog</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/security">security</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/authentication">authentication</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/passwords">passwords</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/technology">technology</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/software%20development">software development</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/user%20experience">user experience</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/opinion">opinion</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/web%20security">web security</a></div><div class="ArticlePreview_summary__Zyb4E"><p>where my words occasionally escape /dev/null</p></div></div></div></div><div class="page_controls__bJjEu"><a class="page_button__2bNeO" href="/url-notes/all/4">newer</a><a class="page_button__2bNeO" href="/url-notes/all/6">older</a></div></div><!--$--><!--/$--><script src="/url-notes/_next/static/chunks/webpack-54d9d4b2aa649176.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n5:I[9665,[],\"OutletBoundary\"]\n7:I[4911,[],\"AsyncMetadataOutlet\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[8393,[],\"\"]\n:HL[\"/url-notes/_next/static/css/f03c873af434c7c6.css\",\"style\"]\n:HL[\"/url-notes/_next/static/css/0e5ea1ea0183b412.css\",\"style\"]\n:HL[\"/url-notes/_next/static/css/7190d9c623ab1fe0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"lyanoQfPS9ZzI0wL9d9zm\",\"p\":\"/url-notes\",\"c\":[\"\",\"all\",\"5\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"all\",{\"children\":[[\"pageIndex\",\"5\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/f03c873af434c7c6.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"all\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"pageIndex\",\"5\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/0e5ea1ea0183b412.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/7190d9c623ab1fe0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$L6\",[\"$\",\"$L7\",null,{\"promise\":\"$@8\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$c\",null,{\"fallback\":null,\"children\":\"$Ld\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"f:I[6874,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"216\",\"static/chunks/app/all/%5BpageIndex%5D/page-680db69597409d20.js\"],\"\"]\n1c:I[8175,[],\"IconMark\"]\n10:T735,"])</script><script>self.__next_f.push([1,"Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity."])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"page_Page__nS49m\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Header_Header__VDN4T\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Header_item__PWJos\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/\",\"children\":\"url-notes\"}]}],[[[\"$\",\"div\",\"separator-0\",{\"className\":\"Header_separator__zF22U\",\"children\":\"|\"}],[\"$\",\"div\",\"item-0\",{\"className\":\"Header_item__PWJos\",\"children\":[\"$\",\"$Lf\",\"0\",{\"href\":\"/all/0\",\"children\":\"all\"}]}]],[[\"$\",\"div\",\"separator-1\",{\"className\":\"Header_separator__zF22U\",\"children\":\"|\"}],[\"$\",\"div\",\"item-1\",{\"className\":\"Header_item__PWJos\",\"children\":[\"$\",\"span\",\"1\",{\"children\":[\"page \",6,\" of \",20]}]}]]]]}],[\"$\",\"div\",null,{\"className\":\"page_content___i7Ut\",\"children\":[\"$\",\"div\",null,{\"className\":\"page_previews__BuBMS\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2506.17871\",\"target\":\"_blank\",\"children\":\"How Alignment Shrinks the Generative Horizon\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/\",\"children\":\"\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Computation%20and%20Language\",\"children\":\"Computation and Language\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Artificial%20Intelligence\",\"children\":\"Artificial Intelligence\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Machine%20Learning\",\"children\":\"Machine Learning\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$10\"}]]}]]}],[\"$\",\"div\",\"1\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2506.21521\",\"target\":\"_blank\",\"children\":\"Potemkin Understanding in Large Language Models\"}]}],\"$L11\",\"$L12\"]}],\"$L13\",\"$L14\",\"$L15\",\"$L16\",\"$L17\",\"$L18\",\"$L19\",\"$L1a\"]}]}],\"$L1b\"]}]\n"])</script><script>self.__next_f.push([1,"8:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"url-notes | all | page 6 of 20\"}],[\"$\",\"link\",\"1\",{\"rel\":\"icon\",\"href\":\"/url-notes/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}],[\"$\",\"$L1c\",\"2\",{}]],\"error\":null,\"digest\":\"$undefined\"}\nd:\"$8:metadata\"\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Computation%20and%20Language\",\"children\":\"Computation and Language\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Artificial%20Intelligence\",\"children\":\"Artificial Intelligence\"}]]}]\n1d:T475,"])</script><script>self.__next_f.push([1,"Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations."])</script><script>self.__next_f.push([1,"12:[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$1d\"}]]}]\n1e:T75d,"])</script><script>self.__next_f.push([1,"https://download.ssrn.com/2025/6/30/4440242.pdf?response-content-disposition=attachment%3B%20filename%3Dssrn-4440242.pdf\u0026X-Amz-Security-Token=IQoJb3JpZ2luX2VjENb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDmvD0kwhxUvwfI51pkPe7LHJvlbunkSaSJ%2F%2FWUHoHLxwIgKvlIsW8iuAAP7s8oth%2F9YaKIKpRZ81hCBzCyI8YDF5oqxQUIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzMDg0NzUzMDEyNTciDPFDIhL%2BP88i2OXZ6SqZBefQZOwOYYLdg%2BqExabceAZ%2BrERHWnxBuYo%2BE3mBpdQ5F230f6aZ0YPQPQLAlDLggStpWurCRPjzdUpiah95RWY5%2Fho%2B%2BIJMQt%2BXlP2zJ0tH%2Bjc8L44sVb5xro0WdeoEbn9kv1HEEgPlyJYK9VnuSWakRRvq2rYBQF9GGYtOSNRKqubOHX3sn0Otqt0i4xN5FuayIg0CPeZtJ2ZgN%2FEH%2FDw6oir%2BvSvn%2FtOXrpYNe74yps2euHd9iiv21zZRIGlGwmKnrd%2F5lsjBUoLnOSd9VAa%2BqKrYp9ivQfcCB7Oqe9Rj4iLjh8TeVCyRqkXwBxbOOhxW8gomEJOcjOJpiTOeNooyzLgR7FZqo2h8ia6aVyhHQPTDvCQgtLLc%2FYQnd%2FQ9qZWwx2%2FbXqeXsNI5Or2qcgPy8Ul4%2F%2FeuLHbFsLA6FnY%2Fz5HbC%2BNoQP3SV3od4tYKPESNObQOMKixm7iq5TIdK1OzaeQhlSpGdFs7GWwiAWqXNDHVVFbdXLDczvSPpEaubfmUUhLiZaZA05HLMpMBrsDXgPfjVxwikIyC20x1NfE2G6diCN5zCaqR5l5Jh4lzKXTOKDZcs8iJs3I4mjCKmRn3BqSuWlaSI4K049EVQvifWBF8Oz42Yub0UXIbns659tipXWgnpe2t9dkVKEzRt4ENmyOi%2FaUHPxeQkigTmXHR0Cmb%2FYagZpfp5knBLBXlCjZxy3OlLDs8qZmvXreW4cBEts7UNy8IQLeuoxBYkr4nbNreSYRpTjw4tcG9zPmUUzuRVSt6VF07W5KJ1jQqTI4aJ4wtmWSgMFopPVGiyduaToM7%2FBxMY5gpV2ZsByt%2BJCObeZrUyRfq2HwHoWC5wGMiOjtsYe%2Bmetmp890mAOxL6VJowA%2Bl9Kh%2FMOnqjcMGOrEB18AnWgBDmlKkSscFIxJOL33ixrSai7YAjhHXpoVSUh3B0Z%2BMO2iO4XxEAzuzYhL9J%2BovhoVJ9GaFJdmMOqrxN1CRHitVibPr0mE0wZyNGqpu%2FjWOg6wvjLuET7Wxt7jxHMzmdpPEO26pBemUMHTSjbGtrC7Asc7wbAusIZ%2Fcuqy8B6MgLkiLi1QOIjBHzY7Vs6p2nu5u%2BjF4%2Fld1vyCVhsoH6J5EoMRRyixpRF4qv3T0\u0026X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Date=20250701T054517Z\u0026X-Amz-SignedHeaders=host\u0026X-Amz-Expires=300\u0026X-Amz-Credential=ASIAUPUUPRWE7AM5UVQA%2F20250701%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Signature=50a47418d1bede25e5f619afb7a74d57c0d0600407a48e401744a96f24f59ed8\u0026abstractId=4440242"])</script><script>self.__next_f.push([1,"13:[\"$\",\"div\",\"2\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"$1e\",\"target\":\"_blank\",\"children\":\"Cancel Culture\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/cancel%20culture\",\"children\":\"cancel culture\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/free%20speech\",\"children\":\"free speech\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/employee%20speech\",\"children\":\"employee speech\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/public%20discourse\",\"children\":\"public discourse\"}],[\"$\",\"$Lf\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/social%20media\",\"children\":\"social media\"}],[\"$\",\"$Lf\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/controversy\",\"children\":\"controversy\"}],[\"$\",\"$Lf\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/censorship\",\"children\":\"censorship\"}],[\"$\",\"$Lf\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/online%20shaming\",\"children\":\"online shaming\"}],[\"$\",\"$Lf\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/deplatforming\",\"children\":\"deplatforming\"}],[\"$\",\"$Lf\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/corporate%20policy\",\"children\":\"corporate policy\"}],[\"$\",\"$Lf\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/political%20polarization\",\"children\":\"political polarization\"}],[\"$\",\"$Lf\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/freedom%20of%20expression\",\"children\":\"freedom of expression\"}],[\"$\",\"$Lf\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/social%20justice\",\"children\":\"social justice\"}],\"$L1f\",\"$L20\"]}],\"$L21\"]}]\n"])</script><script>self.__next_f.push([1,"22:T5a6,"])</script><script>self.__next_f.push([1,"Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations."])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",\"3\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://yosefk.com/blog/llms-arent-world-models.htmlhttps://arxiv.org/pdf/2505.17117\",\"target\":\"_blank\",\"children\":\"From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Computation%20and%20Language\",\"children\":\"Computation and Language\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Artificial%20Intelligence\",\"children\":\"Artificial Intelligence\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Information%20Theory\",\"children\":\"Information Theory\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/Information%20Theory\",\"children\":\"Information Theory\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$22\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"div\",\"4\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://www.theregister.com/2025/03/29/malware_obscure_languages/\",\"target\":\"_blank\",\"children\":\"Malware in Lisp? Now you're just being cruel\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/malware\",\"children\":\"malware\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/cybersecurity\",\"children\":\"cybersecurity\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/information%20security\",\"children\":\"information security\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/lisp\",\"children\":\"lisp\"}],[\"$\",\"$Lf\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/haskell\",\"children\":\"haskell\"}],[\"$\",\"$Lf\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/delphi\",\"children\":\"delphi\"}],[\"$\",\"$Lf\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/programming%20languages\",\"children\":\"programming languages\"}],[\"$\",\"$Lf\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/evasion%20techniques\",\"children\":\"evasion techniques\"}],[\"$\",\"$Lf\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/threat%20detection\",\"children\":\"threat detection\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\": Miscreants warming to Delphi, Haskell, and the like to evade detection\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"div\",\"5\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://stackbench.ai/\",\"target\":\"_blank\",\"children\":\"Analyze how well coding agents use your libraries and frameworks\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ai\",\"children\":\"ai\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/llm\",\"children\":\"llm\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/coding%20agents\",\"children\":\"coding agents\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/software%20development\",\"children\":\"software development\"}],[\"$\",\"$Lf\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/library%20development\",\"children\":\"library development\"}],[\"$\",\"$Lf\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/frameworks\",\"children\":\"frameworks\"}],[\"$\",\"$Lf\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/api%20design\",\"children\":\"api design\"}],[\"$\",\"$Lf\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/developer%20experience\",\"children\":\"developer experience\"}],[\"$\",\"$Lf\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/dx\",\"children\":\"dx\"}],[\"$\",\"$Lf\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/documentation\",\"children\":\"documentation\"}],[\"$\",\"$Lf\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/testing\",\"children\":\"testing\"}],[\"$\",\"$Lf\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/agent-friendliness\",\"children\":\"agent-friendliness\"}],[\"$\",\"$Lf\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ai-friendliness\",\"children\":\"ai-friendliness\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"Test your library's agent-friendliness by analyzing how well coding agents use your documentation\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"div\",\"6\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/hasktorch/hasktorch\",\"target\":\"_blank\",\"children\":\"hasktorch/hasktorch\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/haskell\",\"children\":\"haskell\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/machine%20learning\",\"children\":\"machine learning\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/deep%20learning\",\"children\":\"deep learning\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/neural%20networks\",\"children\":\"neural networks\"}],[\"$\",\"$Lf\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/pytorch\",\"children\":\"pytorch\"}],[\"$\",\"$Lf\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/libtorch\",\"children\":\"libtorch\"}],[\"$\",\"$Lf\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/tensors\",\"children\":\"tensors\"}],[\"$\",\"$Lf\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/library\",\"children\":\"library\"}],[\"$\",\"$Lf\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ffi\",\"children\":\"ffi\"}],[\"$\",\"$Lf\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/c%2B%2B\",\"children\":\"c++\"}],[\"$\",\"$Lf\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/cuda\",\"children\":\"cuda\"}],[\"$\",\"$Lf\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpu\",\"children\":\"gpu\"}],[\"$\",\"$Lf\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/scientific%20computing\",\"children\":\"scientific computing\"}],[\"$\",\"$Lf\",\"13\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/code%20generation\",\"children\":\"code generation\"}],[\"$\",\"$Lf\",\"14\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/hasktorch\",\"children\":\"hasktorch\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Hasktorch:\"}],\" A Haskell library for tensors and neural networks.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Core:\"}],\" It is a community project that wraps the C++ backend of PyTorch (libtorch).\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Status:\"}],\" The project is in active development (currently version 0.2) and the API is subject to change.\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Installation:\"}],\" Provides setup guides for multiple environments:\",\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"OS:\"}],\" Linux, macOS, NixOS, Docker.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Build Tools:\"}],\" \",[\"$\",\"code\",\"code-0\",{\"children\":\"cabal\"}],\" and \",[\"$\",\"code\",\"code-1\",{\"children\":\"stack\"}],\".\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Hardware:\"}],\" CPU and NVIDIA CUDA.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Project Structure:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"code\",\"code-0\",{\"children\":\"hasktorch/\"}],\": High-level, user-facing API.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"code\",\"code-0\",{\"children\":\"libtorch-ffi/\"}],\": Low-level Foreign Function Interface (FFI) bindings to libtorch.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"code\",\"code-0\",{\"children\":\"codegen/\"}],\": A code generation module that parses PyTorch's \",[\"$\",\"code\",\"code-1\",{\"children\":\"Declarations.yaml\"}],\" to produce the FFI bindings.\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"code\",\"code-0\",{\"children\":\"examples/\"}],\": Contains example models and usage.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"div\",\"7\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://www.theverge.com/24167865/google-zero-search-crash-housefresh-ai-overviews-traffic-data-audience\",\"target\":\"_blank\",\"children\":\"Google Zero is here â now what?\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/technology\",\"children\":\"technology\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/internet\",\"children\":\"internet\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/web\",\"children\":\"web\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/search\",\"children\":\"search\"}],[\"$\",\"$Lf\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/search%20engines\",\"children\":\"search engines\"}],[\"$\",\"$Lf\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/google\",\"children\":\"google\"}],[\"$\",\"$Lf\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/platforms\",\"children\":\"platforms\"}],[\"$\",\"$Lf\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/web%20platforms\",\"children\":\"web platforms\"}],[\"$\",\"$Lf\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/future%20of%20technology\",\"children\":\"future of technology\"}],[\"$\",\"$Lf\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/technological%20change\",\"children\":\"technological change\"}],[\"$\",\"$Lf\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/platform%20shifts\",\"children\":\"platform shifts\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"Search is a foundational platform the modern web was built on. But all platforms come and go â so whatâs next?\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"23:T411,"])</script><script>self.__next_f.push([1,"With Gaussian Elimination as a representative family of numerical and symbolic algorithms, we use multi-stage programming, monads and Ocaml's advanced module system to demonstrate the complete elimination of the abstraction overhead while avoiding any inspection of the generated code. We parameterize our Gaussian Elimination code to a great extent (over domain, matrix representations, determinant tracking, pivoting policies, result types, etc) at no run-time cost. Because the resulting code is generated just right and not changed afterwards, we enjoy MetaOCaml's guaranty that the generated code is well-typed. We further demonstrate that various abstraction parameters (aspects) can be made orthogonal and compositional, even in the presence of name-generation for temporaries and other bindings and âinterleavingâ of aspects. We also show how to encode some domain-specific knowledge so that âclearly wrongâ compositions can be statically rejected by the compiler when processing the generator rather than the generated code."])</script><script>self.__next_f.push([1,"19:[\"$\",\"div\",\"8\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://dl.acm.org/doi/10.1007/11561347_18\",\"target\":\"_blank\",\"children\":\"Multi-stage programming with functors and monads | Proceedings of the 4th international conference on Generative Programming and Component Engineering\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/multi-stage%20programming\",\"children\":\"multi-stage programming\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/functional%20programming\",\"children\":\"functional programming\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ocaml\",\"children\":\"ocaml\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/metaocaml\",\"children\":\"metaocaml\"}],[\"$\",\"$Lf\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/monads\",\"children\":\"monads\"}],[\"$\",\"$Lf\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/functors\",\"children\":\"functors\"}],[\"$\",\"$Lf\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/generative%20programming\",\"children\":\"generative programming\"}],[\"$\",\"$Lf\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/metaprogramming\",\"children\":\"metaprogramming\"}],[\"$\",\"$Lf\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/compilers\",\"children\":\"compilers\"}],[\"$\",\"$Lf\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/type%20systems\",\"children\":\"type systems\"}],[\"$\",\"$Lf\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gaussian%20elimination\",\"children\":\"gaussian elimination\"}],[\"$\",\"$Lf\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/numerical%20algorithms\",\"children\":\"numerical algorithms\"}],[\"$\",\"$Lf\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/symbolic%20algorithms\",\"children\":\"symbolic algorithms\"}],[\"$\",\"$Lf\",\"13\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/performance%20optimization\",\"children\":\"performance optimization\"}],[\"$\",\"$Lf\",\"14\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/abstraction%20overhead\",\"children\":\"abstraction overhead\"}],[\"$\",\"$Lf\",\"15\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/code%20generation\",\"children\":\"code generation\"}],[\"$\",\"$Lf\",\"16\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/aspect-oriented%20programming\",\"children\":\"aspect-oriented programming\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$23\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1a:[\"$\",\"div\",\"9\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://blog.danielh.cc/blog/passwords\",\"target\":\"_blank\",\"children\":\"We replaced passwords with something worse | Blog\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$Lf\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/security\",\"children\":\"security\"}],[\"$\",\"$Lf\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/authentication\",\"children\":\"authentication\"}],[\"$\",\"$Lf\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/passwords\",\"children\":\"passwords\"}],[\"$\",\"$Lf\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/technology\",\"children\":\"technology\"}],[\"$\",\"$Lf\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/software%20development\",\"children\":\"software development\"}],[\"$\",\"$Lf\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/user%20experience\",\"children\":\"user experience\"}],[\"$\",\"$Lf\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/opinion\",\"children\":\"opinion\"}],[\"$\",\"$Lf\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/web%20security\",\"children\":\"web security\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"where my words occasionally escape /dev/null\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1b:[\"$\",\"div\",null,{\"className\":\"page_controls__bJjEu\",\"children\":[[\"$\",\"$Lf\",null,{\"className\":\"page_button__2bNeO\",\"href\":\"/all/4\",\"children\":\"newer\"}],[\"$\",\"$Lf\",null,{\"className\":\"page_button__2bNeO\",\"href\":\"/all/6\",\"children\":\"older\"}]]}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"$Lf\",\"13\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/political%20correctness\",\"children\":\"political correctness\"}]\n20:[\"$\",\"$Lf\",\"14\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/online%20activism\",\"children\":\"online activism\"}]\n21:[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":\"\u003cdiv\u003e\\n \u003cdiv\u003e\\n  Public controversies over employee speech where individuals face backlash for expressing views deemed controversial,Â \u003cspan\u003eunpopular, or mis\"}]\n"])</script></body></html>