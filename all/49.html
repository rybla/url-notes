<!DOCTYPE html><!--OILQaBsGalePD9Px2K_Oh--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/url-notes/_next/static/css/9881504be370c91a.css" data-precedence="next"/><link rel="stylesheet" href="/url-notes/_next/static/css/b4a799145eafaf0f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/url-notes/_next/static/chunks/webpack-70887be7d164bff6.js"/><script src="/url-notes/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/url-notes/_next/static/chunks/964-a29425d4972030f1.js" async=""></script><script src="/url-notes/_next/static/chunks/main-app-e4d4697bcd6cfe75.js" async=""></script><script src="/url-notes/_next/static/chunks/874-437a265a67d6cfee.js" async=""></script><script src="/url-notes/_next/static/chunks/app/all/%5BpageIndex%5D/page-521ba4b8b4cd9408.js" async=""></script><title>url-notes | all | page 50 of 65</title><link rel="icon" href="/url-notes/favicon.ico" type="image/x-icon" sizes="256x256"/><script src="/url-notes/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><div class="AppPage_AppPage__MciWo"><div class="AppPage_toolbar__H52v2"><div class="AppPage_navigation__Luced"><div class="AppPage_item__vUL6b"><a class="LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH" href="/url-notes">index</a></div><div class="AppPage_item__vUL6b">‚Üê</div><div class="AppPage_item__vUL6b"><a class="LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH" href="/url-notes/all/0">all</a></div></div><div class="AppPage_cornerstone__p6Wox"></div></div><div class="AppPage_main__PIVFu"><div class="page_previews__BuBMS"><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2406.05946">Safety Alignment Should Be Made More Than Just a Few Tokens Deep</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2024-06-09</div><div class="ArticlePreview_summary__Zyb4E"><p>The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model&#x27;s generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2406.05946">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2505.04741">When Bad Data Leads to Good Models</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-05-07</div><div class="ArticlePreview_summary__Zyb4E"><p>In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we re-examine the notion of &quot;quality&quot; from the perspective of pre- and post-training co-design. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model&#x27;s output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2505.04741">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2306.11644">Textbooks Are All You Need</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2023-06-20</div><div class="ArticlePreview_summary__Zyb4E"><p>We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality&quot; data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2306.11644">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf">Redirecting‚Ä¶</a></div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li>This paper details a randomized controlled trial (RCT) investigating the impact of early-2025 AI tools on the productivity of experienced open-source developers.</li>
<li><strong>Methodology:</strong> The study involved 16 developers with an average of 5 years of experience on specific, mature open-source projects. They completed 246 tasks, with each task randomly assigned to either allow or disallow the use of AI tools like Cursor Pro and Claude 3.5/3.7 Sonnet.</li>
<li><strong>Expectation vs. Reality:</strong> Developers predicted that using AI would decrease task completion time by 24% before the study and estimated a 20% time reduction after the study.</li>
<li><strong>Core Finding:</strong> Contrary to expectations from both developers and experts, allowing AI tools <em>increased</em> task completion time by 19%.</li>
<li><strong>Analysis:</strong> The authors investigated 20 different properties of the experimental setting that could explain this slowdown, but the effect remained robust across their analyses, suggesting it is not primarily an artifact of the experimental design.</li>
</ul></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2412.09871">Byte Latent Transformer: Patches Scale Better Than Tokens</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2024-12-13</div><div class="ArticlePreview_summary__Zyb4E"><p>We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2412.09871">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2506.18777">Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-06-23</div><div class="ArticlePreview_summary__Zyb4E"><p>Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2506.18777">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2506.17871">How Alignment Shrinks the Generative Horizon</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-06-21</div><div class="ArticlePreview_summary__Zyb4E"><p>Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model&#x27;s output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model&#x27;s output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model&#x27;s behavior, but instead steers it toward stylistic tokens (e.g., &quot;Sure&quot;) that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2506.17871">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2506.21521">Potemkin Understanding in Large Language Models</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-06-26</div><div class="ArticlePreview_summary__Zyb4E"><p>Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM&#x27;s capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2506.21521">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://download.ssrn.com/2025/6/30/4440242.pdf?response-content-disposition=attachment%3B%20filename%3Dssrn-4440242.pdf&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjENb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDmvD0kwhxUvwfI51pkPe7LHJvlbunkSaSJ%2F%2FWUHoHLxwIgKvlIsW8iuAAP7s8oth%2F9YaKIKpRZ81hCBzCyI8YDF5oqxQUIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzMDg0NzUzMDEyNTciDPFDIhL%2BP88i2OXZ6SqZBefQZOwOYYLdg%2BqExabceAZ%2BrERHWnxBuYo%2BE3mBpdQ5F230f6aZ0YPQPQLAlDLggStpWurCRPjzdUpiah95RWY5%2Fho%2B%2BIJMQt%2BXlP2zJ0tH%2Bjc8L44sVb5xro0WdeoEbn9kv1HEEgPlyJYK9VnuSWakRRvq2rYBQF9GGYtOSNRKqubOHX3sn0Otqt0i4xN5FuayIg0CPeZtJ2ZgN%2FEH%2FDw6oir%2BvSvn%2FtOXrpYNe74yps2euHd9iiv21zZRIGlGwmKnrd%2F5lsjBUoLnOSd9VAa%2BqKrYp9ivQfcCB7Oqe9Rj4iLjh8TeVCyRqkXwBxbOOhxW8gomEJOcjOJpiTOeNooyzLgR7FZqo2h8ia6aVyhHQPTDvCQgtLLc%2FYQnd%2FQ9qZWwx2%2FbXqeXsNI5Or2qcgPy8Ul4%2F%2FeuLHbFsLA6FnY%2Fz5HbC%2BNoQP3SV3od4tYKPESNObQOMKixm7iq5TIdK1OzaeQhlSpGdFs7GWwiAWqXNDHVVFbdXLDczvSPpEaubfmUUhLiZaZA05HLMpMBrsDXgPfjVxwikIyC20x1NfE2G6diCN5zCaqR5l5Jh4lzKXTOKDZcs8iJs3I4mjCKmRn3BqSuWlaSI4K049EVQvifWBF8Oz42Yub0UXIbns659tipXWgnpe2t9dkVKEzRt4ENmyOi%2FaUHPxeQkigTmXHR0Cmb%2FYagZpfp5knBLBXlCjZxy3OlLDs8qZmvXreW4cBEts7UNy8IQLeuoxBYkr4nbNreSYRpTjw4tcG9zPmUUzuRVSt6VF07W5KJ1jQqTI4aJ4wtmWSgMFopPVGiyduaToM7%2FBxMY5gpV2ZsByt%2BJCObeZrUyRfq2HwHoWC5wGMiOjtsYe%2Bmetmp890mAOxL6VJowA%2Bl9Kh%2FMOnqjcMGOrEB18AnWgBDmlKkSscFIxJOL33ixrSai7YAjhHXpoVSUh3B0Z%2BMO2iO4XxEAzuzYhL9J%2BovhoVJ9GaFJdmMOqrxN1CRHitVibPr0mE0wZyNGqpu%2FjWOg6wvjLuET7Wxt7jxHMzmdpPEO26pBemUMHTSjbGtrC7Asc7wbAusIZ%2Fcuqy8B6MgLkiLi1QOIjBHzY7Vs6p2nu5u%2BjF4%2Fld1vyCVhsoH6J5EoMRRyixpRF4qv3T0&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20250701T054517Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAUPUUPRWE7AM5UVQA%2F20250701%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=50a47418d1bede25e5f619afb7a74d57c0d0600407a48e401744a96f24f59ed8&amp;abstractId=4440242">Cancel Culture</a></div><div class="ArticlePreview_summary__Zyb4E">&lt;div&gt;
 &lt;div&gt;
  Public controversies over employee speech where individuals face backlash for expressing views deemed controversial,¬†&lt;span&gt;unpopular, or mis</div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://download.ssrn.com/2025/6/30/4440242.pdf?response-content-disposition=attachment%3B%20filename%3Dssrn-4440242.pdf&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjENb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDmvD0kwhxUvwfI51pkPe7LHJvlbunkSaSJ%2F%2FWUHoHLxwIgKvlIsW8iuAAP7s8oth%2F9YaKIKpRZ81hCBzCyI8YDF5oqxQUIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzMDg0NzUzMDEyNTciDPFDIhL%2BP88i2OXZ6SqZBefQZOwOYYLdg%2BqExabceAZ%2BrERHWnxBuYo%2BE3mBpdQ5F230f6aZ0YPQPQLAlDLggStpWurCRPjzdUpiah95RWY5%2Fho%2B%2BIJMQt%2BXlP2zJ0tH%2Bjc8L44sVb5xro0WdeoEbn9kv1HEEgPlyJYK9VnuSWakRRvq2rYBQF9GGYtOSNRKqubOHX3sn0Otqt0i4xN5FuayIg0CPeZtJ2ZgN%2FEH%2FDw6oir%2BvSvn%2FtOXrpYNe74yps2euHd9iiv21zZRIGlGwmKnrd%2F5lsjBUoLnOSd9VAa%2BqKrYp9ivQfcCB7Oqe9Rj4iLjh8TeVCyRqkXwBxbOOhxW8gomEJOcjOJpiTOeNooyzLgR7FZqo2h8ia6aVyhHQPTDvCQgtLLc%2FYQnd%2FQ9qZWwx2%2FbXqeXsNI5Or2qcgPy8Ul4%2F%2FeuLHbFsLA6FnY%2Fz5HbC%2BNoQP3SV3od4tYKPESNObQOMKixm7iq5TIdK1OzaeQhlSpGdFs7GWwiAWqXNDHVVFbdXLDczvSPpEaubfmUUhLiZaZA05HLMpMBrsDXgPfjVxwikIyC20x1NfE2G6diCN5zCaqR5l5Jh4lzKXTOKDZcs8iJs3I4mjCKmRn3BqSuWlaSI4K049EVQvifWBF8Oz42Yub0UXIbns659tipXWgnpe2t9dkVKEzRt4ENmyOi%2FaUHPxeQkigTmXHR0Cmb%2FYagZpfp5knBLBXlCjZxy3OlLDs8qZmvXreW4cBEts7UNy8IQLeuoxBYkr4nbNreSYRpTjw4tcG9zPmUUzuRVSt6VF07W5KJ1jQqTI4aJ4wtmWSgMFopPVGiyduaToM7%2FBxMY5gpV2ZsByt%2BJCObeZrUyRfq2HwHoWC5wGMiOjtsYe%2Bmetmp890mAOxL6VJowA%2Bl9Kh%2FMOnqjcMGOrEB18AnWgBDmlKkSscFIxJOL33ixrSai7YAjhHXpoVSUh3B0Z%2BMO2iO4XxEAzuzYhL9J%2BovhoVJ9GaFJdmMOqrxN1CRHitVibPr0mE0wZyNGqpu%2FjWOg6wvjLuET7Wxt7jxHMzmdpPEO26pBemUMHTSjbGtrC7Asc7wbAusIZ%2Fcuqy8B6MgLkiLi1QOIjBHzY7Vs6p2nu5u%2BjF4%2Fld1vyCVhsoH6J5EoMRRyixpRF4qv3T0&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20250701T054517Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAUPUUPRWE7AM5UVQA%2F20250701%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=50a47418d1bede25e5f619afb7a74d57c0d0600407a48e401744a96f24f59ed8&amp;abstractId=4440242">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-08-14</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://yosefk.com/blog/llms-arent-world-models.htmlhttps://arxiv.org/pdf/2505.17117">From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-05-21</div><div class="ArticlePreview_summary__Zyb4E"><p>Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://yosefk.com/blog/llms-arent-world-models.htmlhttps://arxiv.org/pdf/2505.17117">üîó</a></div></div></div><div class="page_toolbar__VAh0U"><div class="page_navigation___3IFr"><a class="LinkButton_LinkButton__nW1G0" href="/url-notes/all/48">newer</a><a class="LinkButton_LinkButton__nW1G0" href="/url-notes/all/50">older</a></div><div class="page_location__WbWIR"><span class="FocusSpan_FocusSpan__MPt_V">50</span>/<!-- -->65</div></div></div></div><!--$--><!--/$--><script src="/url-notes/_next/static/chunks/webpack-70887be7d164bff6.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n5:I[9665,[],\"OutletBoundary\"]\n7:I[4911,[],\"AsyncMetadataOutlet\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[8393,[],\"\"]\n:HL[\"/url-notes/_next/static/css/9881504be370c91a.css\",\"style\"]\n:HL[\"/url-notes/_next/static/css/b4a799145eafaf0f.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"OILQaBsGalePD9Px2K-Oh\",\"p\":\"/url-notes\",\"c\":[\"\",\"all\",\"49\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"all\",{\"children\":[[\"pageIndex\",\"49\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/9881504be370c91a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"$undefined\",\"children\":[\"$\",\"body\",null,{\"className\":\"$undefined\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"all\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"pageIndex\",\"49\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/b4a799145eafaf0f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$L6\",[\"$\",\"$L7\",null,{\"promise\":\"$@8\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$c\",null,{\"fallback\":null,\"children\":\"$Ld\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"f:I[6874,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"216\",\"static/chunks/app/all/%5BpageIndex%5D/page-521ba4b8b4cd9408.js\"],\"\"]\n24:I[8175,[],\"IconMark\"]\n10:T5a6,"])</script><script>self.__next_f.push([1,"The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep."])</script><script>self.__next_f.push([1,"11:T4ed,"])</script><script>self.__next_f.push([1,"In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we re-examine the notion of \"quality\" from the perspective of pre- and post-training co-design. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models."])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"AppPage_AppPage__MciWo\",\"children\":[[\"$\",\"div\",null,{\"className\":\"AppPage_toolbar__H52v2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"AppPage_navigation__Luced\",\"children\":[[\"$\",\"div\",\"item-0\",{\"className\":\"AppPage_item__vUL6b\",\"children\":[\"$\",\"$Lf\",\"0\",{\"className\":\"LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH\",\"href\":\"/\",\"target\":\"$undefined\",\"children\":\"index\"}]}],[\"$\",\"div\",\"sep-0\",{\"className\":\"AppPage_item__vUL6b\",\"children\":\"‚Üê\"}],[\"$\",\"div\",\"item-1\",{\"className\":\"AppPage_item__vUL6b\",\"children\":[\"$\",\"$Lf\",\"0\",{\"className\":\"LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH\",\"href\":\"/all/0\",\"target\":\"$undefined\",\"children\":\"all\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"AppPage_cornerstone__p6Wox\"}]]}],[\"$\",\"div\",null,{\"className\":\"AppPage_main__PIVFu\",\"children\":[[\"$\",\"div\",null,{\"className\":\"page_previews__BuBMS\",\"children\":[[\"$\",\"div\",\"sep-0\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}],[\"$\",\"div\",\"0\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2406.05946\",\"target\":\"_blank\",\"children\":\"Safety Alignment Should Be Made More Than Just a Few Tokens Deep\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2024-06-09\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$10\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2406.05946\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}],[\"$\",\"div\",\"sep-1\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}],[\"$\",\"div\",\"1\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2505.04741\",\"target\":\"_blank\",\"children\":\"When Bad Data Leads to Good Models\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-05-07\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$11\"}]]}],\"$L12\"]}],\"$L13\",\"$L14\",\"$L15\",\"$L16\",\"$L17\",\"$L18\",\"$L19\",\"$L1a\",\"$L1b\",\"$L1c\",\"$L1d\",\"$L1e\",\"$L1f\",\"$L20\",\"$L21\",\"$L22\"]}],\"$L23\"]}]]}]\n"])</script><script>self.__next_f.push([1,"8:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"url-notes | all | page 50 of 65\"}],[\"$\",\"link\",\"1\",{\"rel\":\"icon\",\"href\":\"/url-notes/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}],[\"$\",\"$L24\",\"2\",{}]],\"error\":null,\"digest\":\"$undefined\"}\nd:\"$8:metadata\"\n"])</script><script>self.__next_f.push([1,"12:[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2505.04741\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]\n13:[\"$\",\"div\",\"sep-2\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",\"2\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2306.11644\",\"target\":\"_blank\",\"children\":\"Textbooks Are All You Need\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2023-06-20\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\\\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2306.11644\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"div\",\"sep-3\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"div\",\"3\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf\",\"target\":\"_blank\",\"children\":\"Redirecting‚Ä¶\"}]}],\"$undefined\",\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"This paper details a randomized controlled trial (RCT) investigating the impact of early-2025 AI tools on the productivity of experienced open-source developers.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Methodology:\"}],\" The study involved 16 developers with an average of 5 years of experience on specific, mature open-source projects. They completed 246 tasks, with each task randomly assigned to either allow or disallow the use of AI tools like Cursor Pro and Claude 3.5/3.7 Sonnet.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Expectation vs. Reality:\"}],\" Developers predicted that using AI would decrease task completion time by 24% before the study and estimated a 20% time reduction after the study.\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Core Finding:\"}],\" Contrary to expectations from both developers and experts, allowing AI tools \",[\"$\",\"em\",\"em-0\",{\"children\":\"increased\"}],\" task completion time by 19%.\"]}],\"\\n\",[\"$\",\"li\",\"li-4\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Analysis:\"}],\" The authors investigated 20 different properties of the experimental setting that could explain this slowdown, but the effect remained robust across their analyses, suggesting it is not primarily an artifact of the experimental design.\"]}],\"\\n\"]}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"div\",\"sep-4\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n25:T41c,"])</script><script>self.__next_f.push([1,"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size."])</script><script>self.__next_f.push([1,"18:[\"$\",\"div\",\"4\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2412.09871\",\"target\":\"_blank\",\"children\":\"Byte Latent Transformer: Patches Scale Better Than Tokens\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2024-12-13\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$25\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2412.09871\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"19:[\"$\",\"div\",\"sep-5\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n26:T67a,"])</script><script>self.__next_f.push([1,"Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles."])</script><script>self.__next_f.push([1,"1a:[\"$\",\"div\",\"5\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2506.18777\",\"target\":\"_blank\",\"children\":\"Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-06-23\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$26\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2506.18777\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"1b:[\"$\",\"div\",\"sep-6\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n27:T735,"])</script><script>self.__next_f.push([1,"Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity."])</script><script>self.__next_f.push([1,"1c:[\"$\",\"div\",\"6\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2506.17871\",\"target\":\"_blank\",\"children\":\"How Alignment Shrinks the Generative Horizon\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-06-21\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$27\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2506.17871\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"1d:[\"$\",\"div\",\"sep-7\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n28:T475,"])</script><script>self.__next_f.push([1,"Large language models (LLMs) are regularly evaluated using benchmark datasets. But what justifies making inferences about an LLM's capabilities based on its answers to a curated set of questions? This paper first introduces a formal framework to address this question. The key is to note that the benchmarks used to test LLMs -- such as AP exams -- are also those used to test people. However, this raises an implication: these benchmarks are only valid tests if LLMs misunderstand concepts in ways that mirror human misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin understanding: the illusion of understanding driven by answers irreconcilable with how any human would interpret a concept. We present two procedures for quantifying the existence of potemkins: one using a specially designed benchmark in three domains, the other using a general procedure that provides a lower-bound on their prevalence. We find that potemkins are ubiquitous across models, tasks, and domains. We also find that these failures reflect not just incorrect understanding, but deeper internal incoherence in concept representations."])</script><script>self.__next_f.push([1,"1e:[\"$\",\"div\",\"7\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2506.21521\",\"target\":\"_blank\",\"children\":\"Potemkin Understanding in Large Language Models\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-06-26\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$28\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2506.21521\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"div\",\"sep-8\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n29:T75d,"])</script><script>self.__next_f.push([1,"https://download.ssrn.com/2025/6/30/4440242.pdf?response-content-disposition=attachment%3B%20filename%3Dssrn-4440242.pdf\u0026X-Amz-Security-Token=IQoJb3JpZ2luX2VjENb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDmvD0kwhxUvwfI51pkPe7LHJvlbunkSaSJ%2F%2FWUHoHLxwIgKvlIsW8iuAAP7s8oth%2F9YaKIKpRZ81hCBzCyI8YDF5oqxQUIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzMDg0NzUzMDEyNTciDPFDIhL%2BP88i2OXZ6SqZBefQZOwOYYLdg%2BqExabceAZ%2BrERHWnxBuYo%2BE3mBpdQ5F230f6aZ0YPQPQLAlDLggStpWurCRPjzdUpiah95RWY5%2Fho%2B%2BIJMQt%2BXlP2zJ0tH%2Bjc8L44sVb5xro0WdeoEbn9kv1HEEgPlyJYK9VnuSWakRRvq2rYBQF9GGYtOSNRKqubOHX3sn0Otqt0i4xN5FuayIg0CPeZtJ2ZgN%2FEH%2FDw6oir%2BvSvn%2FtOXrpYNe74yps2euHd9iiv21zZRIGlGwmKnrd%2F5lsjBUoLnOSd9VAa%2BqKrYp9ivQfcCB7Oqe9Rj4iLjh8TeVCyRqkXwBxbOOhxW8gomEJOcjOJpiTOeNooyzLgR7FZqo2h8ia6aVyhHQPTDvCQgtLLc%2FYQnd%2FQ9qZWwx2%2FbXqeXsNI5Or2qcgPy8Ul4%2F%2FeuLHbFsLA6FnY%2Fz5HbC%2BNoQP3SV3od4tYKPESNObQOMKixm7iq5TIdK1OzaeQhlSpGdFs7GWwiAWqXNDHVVFbdXLDczvSPpEaubfmUUhLiZaZA05HLMpMBrsDXgPfjVxwikIyC20x1NfE2G6diCN5zCaqR5l5Jh4lzKXTOKDZcs8iJs3I4mjCKmRn3BqSuWlaSI4K049EVQvifWBF8Oz42Yub0UXIbns659tipXWgnpe2t9dkVKEzRt4ENmyOi%2FaUHPxeQkigTmXHR0Cmb%2FYagZpfp5knBLBXlCjZxy3OlLDs8qZmvXreW4cBEts7UNy8IQLeuoxBYkr4nbNreSYRpTjw4tcG9zPmUUzuRVSt6VF07W5KJ1jQqTI4aJ4wtmWSgMFopPVGiyduaToM7%2FBxMY5gpV2ZsByt%2BJCObeZrUyRfq2HwHoWC5wGMiOjtsYe%2Bmetmp890mAOxL6VJowA%2Bl9Kh%2FMOnqjcMGOrEB18AnWgBDmlKkSscFIxJOL33ixrSai7YAjhHXpoVSUh3B0Z%2BMO2iO4XxEAzuzYhL9J%2BovhoVJ9GaFJdmMOqrxN1CRHitVibPr0mE0wZyNGqpu%2FjWOg6wvjLuET7Wxt7jxHMzmdpPEO26pBemUMHTSjbGtrC7Asc7wbAusIZ%2Fcuqy8B6MgLkiLi1QOIjBHzY7Vs6p2nu5u%2BjF4%2Fld1vyCVhsoH6J5EoMRRyixpRF4qv3T0\u0026X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Date=20250701T054517Z\u0026X-Amz-SignedHeaders=host\u0026X-Amz-Expires=300\u0026X-Amz-Credential=ASIAUPUUPRWE7AM5UVQA%2F20250701%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Signature=50a47418d1bede25e5f619afb7a74d57c0d0600407a48e401744a96f24f59ed8\u0026abstractId=4440242"])</script><script>self.__next_f.push([1,"2a:T75d,"])</script><script>self.__next_f.push([1,"https://download.ssrn.com/2025/6/30/4440242.pdf?response-content-disposition=attachment%3B%20filename%3Dssrn-4440242.pdf\u0026X-Amz-Security-Token=IQoJb3JpZ2luX2VjENb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDmvD0kwhxUvwfI51pkPe7LHJvlbunkSaSJ%2F%2FWUHoHLxwIgKvlIsW8iuAAP7s8oth%2F9YaKIKpRZ81hCBzCyI8YDF5oqxQUIz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAEGgwzMDg0NzUzMDEyNTciDPFDIhL%2BP88i2OXZ6SqZBefQZOwOYYLdg%2BqExabceAZ%2BrERHWnxBuYo%2BE3mBpdQ5F230f6aZ0YPQPQLAlDLggStpWurCRPjzdUpiah95RWY5%2Fho%2B%2BIJMQt%2BXlP2zJ0tH%2Bjc8L44sVb5xro0WdeoEbn9kv1HEEgPlyJYK9VnuSWakRRvq2rYBQF9GGYtOSNRKqubOHX3sn0Otqt0i4xN5FuayIg0CPeZtJ2ZgN%2FEH%2FDw6oir%2BvSvn%2FtOXrpYNe74yps2euHd9iiv21zZRIGlGwmKnrd%2F5lsjBUoLnOSd9VAa%2BqKrYp9ivQfcCB7Oqe9Rj4iLjh8TeVCyRqkXwBxbOOhxW8gomEJOcjOJpiTOeNooyzLgR7FZqo2h8ia6aVyhHQPTDvCQgtLLc%2FYQnd%2FQ9qZWwx2%2FbXqeXsNI5Or2qcgPy8Ul4%2F%2FeuLHbFsLA6FnY%2Fz5HbC%2BNoQP3SV3od4tYKPESNObQOMKixm7iq5TIdK1OzaeQhlSpGdFs7GWwiAWqXNDHVVFbdXLDczvSPpEaubfmUUhLiZaZA05HLMpMBrsDXgPfjVxwikIyC20x1NfE2G6diCN5zCaqR5l5Jh4lzKXTOKDZcs8iJs3I4mjCKmRn3BqSuWlaSI4K049EVQvifWBF8Oz42Yub0UXIbns659tipXWgnpe2t9dkVKEzRt4ENmyOi%2FaUHPxeQkigTmXHR0Cmb%2FYagZpfp5knBLBXlCjZxy3OlLDs8qZmvXreW4cBEts7UNy8IQLeuoxBYkr4nbNreSYRpTjw4tcG9zPmUUzuRVSt6VF07W5KJ1jQqTI4aJ4wtmWSgMFopPVGiyduaToM7%2FBxMY5gpV2ZsByt%2BJCObeZrUyRfq2HwHoWC5wGMiOjtsYe%2Bmetmp890mAOxL6VJowA%2Bl9Kh%2FMOnqjcMGOrEB18AnWgBDmlKkSscFIxJOL33ixrSai7YAjhHXpoVSUh3B0Z%2BMO2iO4XxEAzuzYhL9J%2BovhoVJ9GaFJdmMOqrxN1CRHitVibPr0mE0wZyNGqpu%2FjWOg6wvjLuET7Wxt7jxHMzmdpPEO26pBemUMHTSjbGtrC7Asc7wbAusIZ%2Fcuqy8B6MgLkiLi1QOIjBHzY7Vs6p2nu5u%2BjF4%2Fld1vyCVhsoH6J5EoMRRyixpRF4qv3T0\u0026X-Amz-Algorithm=AWS4-HMAC-SHA256\u0026X-Amz-Date=20250701T054517Z\u0026X-Amz-SignedHeaders=host\u0026X-Amz-Expires=300\u0026X-Amz-Credential=ASIAUPUUPRWE7AM5UVQA%2F20250701%2Fus-east-1%2Fs3%2Faws4_request\u0026X-Amz-Signature=50a47418d1bede25e5f619afb7a74d57c0d0600407a48e401744a96f24f59ed8\u0026abstractId=4440242"])</script><script>self.__next_f.push([1,"20:[\"$\",\"div\",\"8\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"$29\",\"target\":\"_blank\",\"children\":\"Cancel Culture\"}]}],\"$undefined\",\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":\"\u003cdiv\u003e\\n \u003cdiv\u003e\\n  Public controversies over employee speech where individuals face backlash for expressing views deemed controversial,¬†\u003cspan\u003eunpopular, or mis\"}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"$2a\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n21:[\"$\",\"div\",\"sep-9\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-08-14\"}]\n2b:T5a6,"])</script><script>self.__next_f.push([1,"Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations."])</script><script>self.__next_f.push([1,"22:[\"$\",\"div\",\"9\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://yosefk.com/blog/llms-arent-world-models.htmlhttps://arxiv.org/pdf/2505.17117\",\"target\":\"_blank\",\"children\":\"From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-05-21\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$2b\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://yosefk.com/blog/llms-arent-world-models.htmlhttps://arxiv.org/pdf/2505.17117\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"23:[\"$\",\"div\",null,{\"className\":\"page_toolbar__VAh0U\",\"children\":[[\"$\",\"div\",null,{\"className\":\"page_navigation___3IFr\",\"children\":[[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"/all/48\",\"target\":\"$undefined\",\"children\":\"newer\"}],[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"/all/50\",\"target\":\"$undefined\",\"children\":\"older\"}]]}],[\"$\",\"div\",null,{\"className\":\"page_location__WbWIR\",\"children\":[[\"$\",\"span\",null,{\"className\":\"FocusSpan_FocusSpan__MPt_V\",\"children\":50}],\"/\",65]}]]}]\n"])</script></body></html>