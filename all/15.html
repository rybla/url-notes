<!DOCTYPE html><!--NRwPT37pU3pEw5vKb3C_O--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/url-notes/_next/static/css/9881504be370c91a.css" data-precedence="next"/><link rel="stylesheet" href="/url-notes/_next/static/css/0fcde6fa80a72931.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/url-notes/_next/static/chunks/webpack-70887be7d164bff6.js"/><script src="/url-notes/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/url-notes/_next/static/chunks/964-a29425d4972030f1.js" async=""></script><script src="/url-notes/_next/static/chunks/main-app-e4d4697bcd6cfe75.js" async=""></script><script src="/url-notes/_next/static/chunks/874-437a265a67d6cfee.js" async=""></script><script src="/url-notes/_next/static/chunks/app/all/%5BpageIndex%5D/page-521ba4b8b4cd9408.js" async=""></script><title>url-notes | all | page 16 of 114</title><link rel="icon" href="/url-notes/favicon.ico" type="image/x-icon" sizes="256x256"/><script src="/url-notes/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><div class="AppPage_AppPage__MciWo"><div class="AppPage_toolbar__H52v2"><div class="AppPage_navigation__Luced"><div class="AppPage_item__vUL6b"><a class="LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH" href="/url-notes">index</a></div><div class="AppPage_item__vUL6b">‚Üê</div><div class="AppPage_item__vUL6b"><a class="LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH" href="/url-notes/all/0">all</a></div></div><div class="AppPage_cornerstone__p6Wox"></div></div><div class="AppPage_main__PIVFu"><div class="page_previews__BuBMS"><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://wharferj.wordpress.com/2012/05/25/ron-cobbs-alien-semiotic-standards/">Ron Cobb‚Äôs Semiotic Standards for Alien‚Ä¶</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2012-05-25</div><div class="ArticlePreview_summary__Zyb4E"><p>With the new Alien film Prometheus coming up in the next few weeks, I watched the first film again recently to remind myself of the story and came across these forgotten gems, referred to on one of‚Ä¶</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://wharferj.wordpress.com/2012/05/25/ron-cobbs-alien-semiotic-standards/">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://repo-sam.inria.fr/nerphys/svbrdf-evaluation/MultiviewSceneMaterials_authors.pdf">An evaluation of SVBRDF Prediction from Generative Image Models for Texturing 3D Scenes</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->D:20250707135331Z</div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li><strong>Objective</strong>: Evaluate the use of generative image models combined with single-image Spatially-Varying Bidirectional Reflectance Distribution Function (SVBRDF) predictors for texturing 3D scenes.</li>
<li><strong>Proposed Pipeline</strong>:<!-- -->
<ol>
<li>Generate multiple conditional RGB image views of a 3D scene geometry using diffusion models.</li>
<li>Apply an SVBRDF prediction network to each generated image to estimate material parameters (e.g., albedo, roughness, metallic).</li>
<li>Merge the resulting multi-view SVBRDF maps into a unified texture atlas for the scene.</li>
</ol>
</li>
<li><strong>Core Challenges Analyzed</strong>:<!-- -->
<ul>
<li><strong>Multi-view Incoherence</strong>: Predictions from individual images may lack consistency, leading to artifacts in the final merged texture atlas.</li>
<li><strong>Prediction Accuracy</strong>: Assessing the performance of SVBRDF predictors, originally trained on photographs, when applied to synthetically generated images.</li>
</ul>
</li>
<li><strong>Key Finding</strong>: The study compares various neural architectures and conditioning strategies, finding that a standard U-Net architecture is surprisingly competitive with more complex designs for achieving both high accuracy and multi-view coherence.</li>
</ul></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://repo-sam.inria.fr/nerphys/svbrdf-evaluation/MultiviewSceneMaterials_authors.pdf">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/abs/2502.11399">FontCraft: Multimodal Font Design Using Interactive Bayesian Optimization</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-02-16</div><div class="ArticlePreview_summary__Zyb4E"><p>Creating new fonts requires a lot of human effort and professional typographic knowledge. Despite the rapid advancements of automatic font generation models, existing methods require users to prepare pre-designed characters with target styles using font-editing software, which poses a problem for non-expert users. To address this limitation, we propose FontCraft, a system that enables font generation without relying on pre-designed characters. Our approach integrates the exploration of a font-style latent space with human-in-the-loop preferential Bayesian optimization and multimodal references, facilitating efficient exploration and enhancing user control. Moreover, FontCraft allows users to revisit previous designs, retracting their earlier choices in the preferential Bayesian optimization process. Once users finish editing the style of a selected character, they can propagate it to the remaining characters and further refine them as needed. The system then generates a complete outline font in OpenType format. We evaluated the effectiveness of FontCraft through a user study comparing it to a baseline interface. Results from both quantitative and qualitative evaluations demonstrate that FontCraft enables non-expert users to design fonts efficiently.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/abs/2502.11399">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2401.04398">Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2024-01-09</div><div class="ArticlePreview_summary__Zyb4E"><p>Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2401.04398">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/pdf/2105.12723">Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2021-05-26</div><div class="ArticlePreview_summary__Zyb4E"><p>Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8$\times$ faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/pdf/2105.12723">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://research.google/blog/introducing-google-research-football-a-novel-reinforcement-learning-environment/">Introducing Google Research Football: A Novel Reinforcement Learning Environment</a></div><div class="ArticlePreview_summary__Zyb4E"><p>Posted by Karol Kurach, Research Lead and Olivier Bachem, Research Scientist, Google Research, Z√ºrich   The goal of reinforcement learning (RL) is ...</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://research.google/blog/introducing-google-research-football-a-novel-reinforcement-learning-environment/">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://aclanthology.org/2020.acl-demos.29.pdf">Usnea: An Authorship Tool for Interactive Fiction using Retrieval Based Semantic Parsing</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->D:20200521164711Z</div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li><strong>Tool:</strong> Usnea, an open-source authoring tool for Interactive Fiction (IF).</li>
<li><strong>Core Technique:</strong> Integrates retrieval-based semantic parsing with traditional branching story structures.</li>
<li><strong>Mechanism:</strong>
<ul>
<li>Uses a nearest neighbor classification variant with inverse semantic similarity as its distance metric (semantic kernel).</li>
<li>Authors define the parser by providing string exemplars paired with class labels.</li>
<li>This avoids the need for formal semantic representations (e.g., FrameNet) or an ML background.</li>
</ul>
</li>
<li><strong>Benefit:</strong> Relaxes the strict lexical options of existing IF systems, allowing for more freeform reader input that is semantically, rather than lexically, matched to author-defined exemplars.</li>
</ul></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://aclanthology.org/2020.acl-demos.29.pdf">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-12</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://arxiv.org/abs/2509.07996">3D and 4D World Modeling: A Survey</a></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-09-04</div><div class="ArticlePreview_summary__Zyb4E"><p>World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models&#x27;&#x27; has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://arxiv.org/abs/2509.07996">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-11</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://www.quantamagazine.org/self-assembly-gets-automated-in-reverse-of-game-of-life-20250910/">Self-Assembly Gets Automated in Reverse of ‚ÄòGame of Life‚Äô | Quanta Magazine</a></div><div class="ArticlePreview_feedName__3OGE7"><span class="ArticlePreview_punctuation__3jr1w">from</span> <span class="ArticlePreview_value__gZoAq">Quanta Magazine</span></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-09-10</div><div class="ArticlePreview_summary__Zyb4E"><p>In cellular automata, simple rules create elaborate structures. Now researchers can start with the structures and reverse-engineer the rules.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://www.quantamagazine.org/self-assembly-gets-automated-in-reverse-of-game-of-life-20250910/">üîó</a></div></div><div class="page_addedDate__qyLFb">2025-09-11</div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://blog.trailofbits.com/2025/09/10/how-sui-move-rethinks-flash-loan-security/">How Sui Move rethinks flash loan security</a></div><div class="ArticlePreview_feedName__3OGE7"><span class="ArticlePreview_punctuation__3jr1w">from</span> <span class="ArticlePreview_value__gZoAq">Trail of Bits</span></div><div class="ArticlePreview_publishedTime__MauIG">published: <!-- -->2025-09-10</div><div class="ArticlePreview_summary__Zyb4E"><p>Sui‚Äôs Move language significantly improves flash loan security by replacing Solidity‚Äôs reliance on callbacks and runtime checks with a ‚Äúhot potato‚Äù model that enforces repayment at the compiler level. This shift makes flash loan security a language guarantee rather than a developer responsibility.</p></div><div class="ArticlePreview_footer__lkeLY"><a class="LinkButton_LinkButton__nW1G0" target="_blank" href="https://blog.trailofbits.com/2025/09/10/how-sui-move-rethinks-flash-loan-security/">üîó</a></div></div></div><div class="page_toolbar__VAh0U"><div class="page_navigation___3IFr"><a class="LinkButton_LinkButton__nW1G0" href="/url-notes/all/14">newer</a><a class="LinkButton_LinkButton__nW1G0" href="/url-notes/all/16">older</a></div><div class="page_location__WbWIR"><span class="FocusSpan_FocusSpan__MPt_V">16</span>/<!-- -->114</div></div></div></div><!--$--><!--/$--><script src="/url-notes/_next/static/chunks/webpack-70887be7d164bff6.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n5:I[9665,[],\"OutletBoundary\"]\n7:I[4911,[],\"AsyncMetadataOutlet\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[8393,[],\"\"]\n:HL[\"/url-notes/_next/static/css/9881504be370c91a.css\",\"style\"]\n:HL[\"/url-notes/_next/static/css/0fcde6fa80a72931.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"NRwPT37pU3pEw5vKb3C-O\",\"p\":\"/url-notes\",\"c\":[\"\",\"all\",\"15\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"all\",{\"children\":[[\"pageIndex\",\"15\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/9881504be370c91a.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"$undefined\",\"children\":[\"$\",\"body\",null,{\"className\":\"$undefined\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"all\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"pageIndex\",\"15\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/0fcde6fa80a72931.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$L6\",[\"$\",\"$L7\",null,{\"promise\":\"$@8\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$c\",null,{\"fallback\":null,\"children\":\"$Ld\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"f:I[6874,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"216\",\"static/chunks/app/all/%5BpageIndex%5D/page-521ba4b8b4cd9408.js\"],\"\"]\n23:I[8175,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"AppPage_AppPage__MciWo\",\"children\":[[\"$\",\"div\",null,{\"className\":\"AppPage_toolbar__H52v2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"AppPage_navigation__Luced\",\"children\":[[\"$\",\"div\",\"item-0\",{\"className\":\"AppPage_item__vUL6b\",\"children\":[\"$\",\"$Lf\",\"0\",{\"className\":\"LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH\",\"href\":\"/\",\"target\":\"$undefined\",\"children\":\"index\"}]}],[\"$\",\"div\",\"sep-0\",{\"className\":\"AppPage_item__vUL6b\",\"children\":\"‚Üê\"}],[\"$\",\"div\",\"item-1\",{\"className\":\"AppPage_item__vUL6b\",\"children\":[\"$\",\"$Lf\",\"0\",{\"className\":\"LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH\",\"href\":\"/all/0\",\"target\":\"$undefined\",\"children\":\"all\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"AppPage_cornerstone__p6Wox\"}]]}],[\"$\",\"div\",null,{\"className\":\"AppPage_main__PIVFu\",\"children\":[[\"$\",\"div\",null,{\"className\":\"page_previews__BuBMS\",\"children\":[[\"$\",\"div\",\"sep-0\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}],[\"$\",\"div\",\"0\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://wharferj.wordpress.com/2012/05/25/ron-cobbs-alien-semiotic-standards/\",\"target\":\"_blank\",\"children\":\"Ron Cobb‚Äôs Semiotic Standards for Alien‚Ä¶\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2012-05-25\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"With the new Alien film Prometheus coming up in the next few weeks, I watched the first film again recently to remind myself of the story and came across these forgotten gems, referred to on one of‚Ä¶\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://wharferj.wordpress.com/2012/05/25/ron-cobbs-alien-semiotic-standards/\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}],[\"$\",\"div\",\"sep-1\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}],[\"$\",\"div\",\"1\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://repo-sam.inria.fr/nerphys/svbrdf-evaluation/MultiviewSceneMaterials_authors.pdf\",\"target\":\"_blank\",\"children\":\"An evaluation of SVBRDF Prediction from Generative Image Models for Texturing 3D Scenes\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"D:20250707135331Z\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Objective\"}],\": Evaluate the use of generative image models combined with single-image Spatially-Varying Bidirectional Reflectance Distribution Function (SVBRDF) predictors for texturing 3D scenes.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Proposed Pipeline\"}],\":\",\"\\n\",[\"$\",\"ol\",\"ol-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Generate multiple conditional RGB image views of a 3D scene geometry using diffusion models.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Apply an SVBRDF prediction network to each generated image to estimate material parameters (e.g., albedo, roughness, metallic).\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Merge the resulting multi-view SVBRDF maps into a unified texture atlas for the scene.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Core Challenges Analyzed\"}],\":\",\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Multi-view Incoherence\"}],\": Predictions from individual images may lack consistency, leading to artifacts in the final merged texture atlas.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Prediction Accuracy\"}],\": Assessing the performance of SVBRDF predictors, originally trained on photographs, when applied to synthetically generated images.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",\"$L10\",\"\\n\"]}]]}],\"$L11\"]}],\"$L12\",\"$L13\",\"$L14\",\"$L15\",\"$L16\",\"$L17\",\"$L18\",\"$L19\",\"$L1a\",\"$L1b\",\"$L1c\",\"$L1d\",\"$L1e\",\"$L1f\",\"$L20\",\"$L21\"]}],\"$L22\"]}]]}]\n"])</script><script>self.__next_f.push([1,"8:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"url-notes | all | page 16 of 114\"}],[\"$\",\"link\",\"1\",{\"rel\":\"icon\",\"href\":\"/url-notes/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}],[\"$\",\"$L23\",\"2\",{}]],\"error\":null,\"digest\":\"$undefined\"}\nd:\"$8:metadata\"\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Key Finding\"}],\": The study compares various neural architectures and conditioning strategies, finding that a standard U-Net architecture is surprisingly competitive with more complex designs for achieving both high accuracy and multi-view coherence.\"]}]\n11:[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://repo-sam.inria.fr/nerphys/svbrdf-evaluation/MultiviewSceneMaterials_authors.pdf\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]\n12:[\"$\",\"div\",\"sep-2\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}]\n24:T4f5,"])</script><script>self.__next_f.push([1,"Creating new fonts requires a lot of human effort and professional typographic knowledge. Despite the rapid advancements of automatic font generation models, existing methods require users to prepare pre-designed characters with target styles using font-editing software, which poses a problem for non-expert users. To address this limitation, we propose FontCraft, a system that enables font generation without relying on pre-designed characters. Our approach integrates the exploration of a font-style latent space with human-in-the-loop preferential Bayesian optimization and multimodal references, facilitating efficient exploration and enhancing user control. Moreover, FontCraft allows users to revisit previous designs, retracting their earlier choices in the preferential Bayesian optimization process. Once users finish editing the style of a selected character, they can propagate it to the remaining characters and further refine them as needed. The system then generates a complete outline font in OpenType format. We evaluated the effectiveness of FontCraft through a user study comparing it to a baseline interface. Results from both quantitative and qualitative evaluations demonstrate that FontCraft enables non-expert users to design fonts efficiently."])</script><script>self.__next_f.push([1,"13:[\"$\",\"div\",\"2\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/abs/2502.11399\",\"target\":\"_blank\",\"children\":\"FontCraft: Multimodal Font Design Using Interactive Bayesian Optimization\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-02-16\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$24\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/abs/2502.11399\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",\"sep-3\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}]\n25:T519,"])</script><script>self.__next_f.push([1,"Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices."])</script><script>self.__next_f.push([1,"15:[\"$\",\"div\",\"3\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2401.04398\",\"target\":\"_blank\",\"children\":\"Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2024-01-09\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$25\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2401.04398\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"div\",\"sep-4\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}]\n26:T4c0,"])</script><script>self.__next_f.push([1,"Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8$\\times$ faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer."])</script><script>self.__next_f.push([1,"17:[\"$\",\"div\",\"4\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2105.12723\",\"target\":\"_blank\",\"children\":\"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2021-05-26\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$26\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/pdf/2105.12723\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"div\",\"sep-5\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}]\n"])</script><script>self.__next_f.push([1,"19:[\"$\",\"div\",\"5\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://research.google/blog/introducing-google-research-football-a-novel-reinforcement-learning-environment/\",\"target\":\"_blank\",\"children\":\"Introducing Google Research Football: A Novel Reinforcement Learning Environment\"}]}],\"$undefined\",\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"Posted by Karol Kurach, Research Lead and Olivier Bachem, Research Scientist, Google Research, Z√ºrich   The goal of reinforcement learning (RL) is ...\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://research.google/blog/introducing-google-research-football-a-novel-reinforcement-learning-environment/\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"1a:[\"$\",\"div\",\"sep-6\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}]\n"])</script><script>self.__next_f.push([1,"1b:[\"$\",\"div\",\"6\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://aclanthology.org/2020.acl-demos.29.pdf\",\"target\":\"_blank\",\"children\":\"Usnea: An Authorship Tool for Interactive Fiction using Retrieval Based Semantic Parsing\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"D:20200521164711Z\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Tool:\"}],\" Usnea, an open-source authoring tool for Interactive Fiction (IF).\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Core Technique:\"}],\" Integrates retrieval-based semantic parsing with traditional branching story structures.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Mechanism:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Uses a nearest neighbor classification variant with inverse semantic similarity as its distance metric (semantic kernel).\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Authors define the parser by providing string exemplars paired with class labels.\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"This avoids the need for formal semantic representations (e.g., FrameNet) or an ML background.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Benefit:\"}],\" Relaxes the strict lexical options of existing IF systems, allowing for more freeform reader input that is semantically, rather than lexically, matched to author-defined exemplars.\"]}],\"\\n\"]}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://aclanthology.org/2020.acl-demos.29.pdf\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"1c:[\"$\",\"div\",\"sep-7\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-12\"}]\n27:T4fe,"])</script><script>self.__next_f.push([1,"World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey"])</script><script>self.__next_f.push([1,"1d:[\"$\",\"div\",\"7\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/abs/2509.07996\",\"target\":\"_blank\",\"children\":\"3D and 4D World Modeling: A Survey\"}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-09-04\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"$27\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://arxiv.org/abs/2509.07996\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"div\",\"sep-8\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-11\"}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"div\",\"8\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://www.quantamagazine.org/self-assembly-gets-automated-in-reverse-of-game-of-life-20250910/\",\"target\":\"_blank\",\"children\":\"Self-Assembly Gets Automated in Reverse of ‚ÄòGame of Life‚Äô | Quanta Magazine\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_feedName__3OGE7\",\"children\":[[\"$\",\"span\",null,{\"className\":\"ArticlePreview_punctuation__3jr1w\",\"children\":\"from\"}],\" \",[\"$\",\"span\",null,{\"className\":\"ArticlePreview_value__gZoAq\",\"children\":\"Quanta Magazine\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-09-10\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"In cellular automata, simple rules create elaborate structures. Now researchers can start with the structures and reverse-engineer the rules.\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://www.quantamagazine.org/self-assembly-gets-automated-in-reverse-of-game-of-life-20250910/\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"20:[\"$\",\"div\",\"sep-9\",{\"className\":\"page_addedDate__qyLFb\",\"children\":\"2025-09-11\"}]\n"])</script><script>self.__next_f.push([1,"21:[\"$\",\"div\",\"9\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"https://blog.trailofbits.com/2025/09/10/how-sui-move-rethinks-flash-loan-security/\",\"target\":\"_blank\",\"children\":\"How Sui Move rethinks flash loan security\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_feedName__3OGE7\",\"children\":[[\"$\",\"span\",null,{\"className\":\"ArticlePreview_punctuation__3jr1w\",\"children\":\"from\"}],\" \",[\"$\",\"span\",null,{\"className\":\"ArticlePreview_value__gZoAq\",\"children\":\"Trail of Bits\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_publishedTime__MauIG\",\"children\":[\"published: \",\"2025-09-10\"]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"p\",\"p-0\",{\"children\":\"Sui‚Äôs Move language significantly improves flash loan security by replacing Solidity‚Äôs reliance on callbacks and runtime checks with a ‚Äúhot potato‚Äù model that enforces repayment at the compiler level. This shift makes flash loan security a language guarantee rather than a developer responsibility.\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_footer__lkeLY\",\"children\":[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"https://blog.trailofbits.com/2025/09/10/how-sui-move-rethinks-flash-loan-security/\",\"target\":\"_blank\",\"children\":\"üîó\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"22:[\"$\",\"div\",null,{\"className\":\"page_toolbar__VAh0U\",\"children\":[[\"$\",\"div\",null,{\"className\":\"page_navigation___3IFr\",\"children\":[[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"/all/14\",\"target\":\"$undefined\",\"children\":\"newer\"}],[\"$\",\"$Lf\",null,{\"className\":\"LinkButton_LinkButton__nW1G0\",\"href\":\"/all/16\",\"target\":\"$undefined\",\"children\":\"older\"}]]}],[\"$\",\"div\",null,{\"className\":\"page_location__WbWIR\",\"children\":[[\"$\",\"span\",null,{\"className\":\"FocusSpan_FocusSpan__MPt_V\",\"children\":16}],\"/\",114]}]]}]\n"])</script></body></html>