1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
5:I[9665,[],"OutletBoundary"]
7:I[4911,[],"AsyncMetadataOutlet"]
9:I[9665,[],"ViewportBoundary"]
b:I[9665,[],"MetadataBoundary"]
c:"$Sreact.suspense"
e:I[8393,[],""]
:HL["/url-notes/_next/static/css/9881504be370c91a.css","style"]
:HL["/url-notes/_next/static/css/b4a799145eafaf0f.css","style"]
0:{"P":null,"b":"FqQD8f6b6kqwkcx59g3VF","p":"/url-notes","c":["","all","70"],"i":false,"f":[[["",{"children":["all",{"children":[["pageIndex","70","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/url-notes/_next/static/css/9881504be370c91a.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"$undefined","children":["$","body",null,{"className":"$undefined","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["all",["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["pageIndex","70","d"],["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L4",[["$","link","0",{"rel":"stylesheet","href":"/url-notes/_next/static/css/b4a799145eafaf0f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L5",null,{"children":["$L6",["$","$L7",null,{"promise":"$@8"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,[["$","$L9",null,{"children":"$La"}],null],["$","$Lb",null,{"children":["$","div",null,{"hidden":true,"children":["$","$c",null,{"fallback":null,"children":"$Ld"}]}]}]]}],false]],"m":"$undefined","G":["$e",[]],"s":false,"S":true}
a:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
6:null
f:I[6874,["874","static/chunks/874-437a265a67d6cfee.js","216","static/chunks/app/all/%5BpageIndex%5D/page-521ba4b8b4cd9408.js"],""]
24:I[8175,[],"IconMark"]
4:["$","div",null,{"className":"AppPage_AppPage__MciWo","children":[["$","div",null,{"className":"AppPage_toolbar__H52v2","children":[["$","div",null,{"className":"AppPage_navigation__Luced","children":[["$","div","item-0",{"className":"AppPage_item__vUL6b","children":["$","$Lf","0",{"className":"LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH","href":"/","target":"$undefined","children":"index"}]}],["$","div","sep-0",{"className":"AppPage_item__vUL6b","children":"‚Üê"}],["$","div","item-1",{"className":"AppPage_item__vUL6b","children":["$","$Lf","0",{"className":"LinkButton_LinkButton__nW1G0 LinkButton_vertical__B1tIH","href":"/all/0","target":"$undefined","children":"all"}]}]]}],["$","div",null,{"className":"AppPage_cornerstone__p6Wox"}]]}],["$","div",null,{"className":"AppPage_main__PIVFu","children":[["$","div",null,{"className":"page_previews__BuBMS","children":[["$","div","sep-0",{"className":"page_addedDate__qyLFb","children":"2025-08-19"}],["$","div","0",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://www.delphiintelligence.io/research/from-data-foundries-to-world-models","target":"_blank","children":"From Data Foundries to World Models"}]}],"$undefined","$undefined",["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","p","p-0",{"children":"Perhaps the most comprehensive overview of today's AI frontier interweaving Data Foundries, Reinforcement Learning, Context Engineering, RL Environments, and World Models into an easily digestible whole"}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://www.delphiintelligence.io/research/from-data-foundries-to-world-models","target":"_blank","children":"üîó"}]}]]}],["$","div","sep-1",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}],["$","div","1",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://mpickering.github.io/papers/multi-stage-programs-in-context.pdf","target":"_blank","children":"Multi-stage Programs in Context"}]}],"$undefined",["$","div",null,{"className":"ArticlePreview_publishedTime__MauIG","children":["published: ","D:20190629211126+01'00'"]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"Core Problem"}],": In multi-stage programming (like Typed Template Haskell), quoting code (",["$","code","code-0",{"children":"JeK"}],") discards implicit, context-derived information such as inferred types, type class instances, and implicit parameters. When the code is later spliced (",["$","code","code-1",{"children":"$$(e)"}],"), this missing information can lead to ambiguity errors or the selection of incorrect type class instances, especially with overlapping instances."]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Proposed Solution"}],":","\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":["Represent quoted terms using a type-preserving format (GHC's ",["$","code","code-0",{"children":"CoreExpr"}],") instead of a purely syntactic AST. This captures the results of type inference and elaboration (e.g., which type class dictionary to use) at the quotation site."]}],"\n",["$","li","li-1",{"children":"Extend the principles of cross-stage persistence to cover implicit information."}],"\n",["$","li","li-2",{"children":["Introduce a ",["$","code","code-0",{"children":"LiftT"}]," type class to persist types across stages, analogous to the ",["$","code","code-1",{"children":"Lift"}]," class for values. This solves ambiguity by ensuring type variables bound in an outer stage are correctly instantiated in an inner stage."]}],"\n",["$","li","li-3",{"children":["Formalize the system with a calculus, ",["$","code","code-0",{"children":"Œª‚áë"}],", that explicitly models staging levels and the elaboration of implicit information."]}],"\n"]}],"\n"]}],"\n","$L10","\n","$L11","\n"]}]]}],"$L12"]}],"$L13","$L14","$L15","$L16","$L17","$L18","$L19","$L1a","$L1b","$L1c","$L1d","$L1e","$L1f","$L20","$L21","$L22"]}],"$L23"]}]]}]
8:{"metadata":[["$","title","0",{"children":"url-notes | all | page 71 of 86"}],["$","link","1",{"rel":"icon","href":"/url-notes/favicon.ico","type":"image/x-icon","sizes":"256x256"}],["$","$L24","2",{}]],"error":null,"digest":"$undefined"}
d:"$8:metadata"
10:["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"Implementation"}],": The paper details an implementation in GHC where typed quotations are serialized as ",["$","code","code-0",{"children":"CoreExpr"}],". This avoids re-typechecking at the splice site, prevents scope extrusion errors, and allows GHC's optimizer to work on the generated code."]}]
11:["$","li","li-3",{"children":[["$","strong","strong-0",{"children":"Key Insight"}],": The meaning of a quoted program fragment is not just its syntax but also the implicit context in which it was created. This context must be preserved in the representation to ensure correctness."]}]
12:["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://mpickering.github.io/papers/multi-stage-programs-in-context.pdf","target":"_blank","children":"üîó"}]}]
13:["$","div","sep-2",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
14:["$","div","2",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://dl.acm.org/doi/10.5555/954186.954190","target":"_blank","children":""}]}],"$undefined","$undefined",["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","p","p-0",{"children":"We use cookies to ensure that we give you the best experience on our website."}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://dl.acm.org/doi/10.5555/954186.954190","target":"_blank","children":"üîó"}]}]]}]
15:["$","div","sep-3",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
16:["$","div","3",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5242171","target":"_blank","children":"The Rising Returns to R&D: Ideas Are not Getting Harder to Find"}]}],"$undefined","$undefined",["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":"<p>R&D investment has grown robustly, yet aggregate productivity growth has stagnated. Is this because ‚Äúideas are getting harder to find‚Äù? This paper uses m"}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5242171","target":"_blank","children":"üîó"}]}]]}]
17:["$","div","sep-4",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
18:["$","div","4",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://arxiv.org/pdf/2001.06358","target":"_blank","children":"Generative Datalog with Continuous Distributions"}]}],"$undefined",["$","div",null,{"className":"ArticlePreview_publishedTime__MauIG","children":["published: ","2020-01-17"]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","p","p-0",{"children":"Arguing for the need to combine declarative and probabilistic programming, B'ar'any et al. (TODS 2017) recently introduced a probabilistic extension of Datalog as a \"purely declarative probabilistic programming language.\" We revisit this language and propose a more principled approach towards defining its semantics based on stochastic kernels and Markov processes - standard notions from probability theory. This allows us to extend the semantics to continuous probability distributions, thereby settling an open problem posed by B'ar'any et al. We show that our semantics is fairly robust, allowing both parallel execution and arbitrary chase orders when evaluating a program. We cast our semantics in the framework of infinite probabilistic databases (Grohe and Lindner, ICDT 2020), and show that the semantics remains meaningful even when the input of a probabilistic Datalog program is an arbitrary probabilistic database."}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://arxiv.org/pdf/2001.06358","target":"_blank","children":"üîó"}]}]]}]
19:["$","div","sep-5",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
1a:["$","div","5",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://arxiv.org/pdf/2506.21734","target":"_blank","children":"Hierarchical Reasoning Model"}]}],"$undefined",["$","div",null,{"className":"ArticlePreview_publishedTime__MauIG","children":["published: ","2025-06-26"]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"Model:"}]," Hierarchical Reasoning Model (HRM), a novel recurrent architecture for complex reasoning tasks."]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Inspiration:"}]," Mimics the hierarchical, multi-timescale processing of the human brain."]}],"\n",["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"Architecture:"}]," Comprises two interdependent recurrent modules:","\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"A high-level module for slow, abstract planning."}],"\n",["$","li","li-1",{"children":"A low-level module for rapid, detailed computation."}],"\n"]}],"\n"]}],"\n",["$","li","li-3",{"children":[["$","strong","strong-0",{"children":"Methodology:"}]," Executes sequential reasoning in a single forward pass without explicit intermediate supervision, pre-training, or Chain-of-Thought (CoT) data."]}],"\n",["$","li","li-4",{"children":[["$","strong","strong-0",{"children":"Efficiency:"}]," Achieves high performance with only 27M parameters and 1000 training samples."]}],"\n",["$","li","li-5",{"children":[["$","strong","strong-0",{"children":"Performance:"}]," Demonstrates near-perfect results on complex Sudoku, optimal pathfinding, and outperforms larger models on the Abstraction and Reasoning Corpus (ARC) benchmark."]}],"\n"]}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://arxiv.org/pdf/2506.21734","target":"_blank","children":"üîó"}]}]]}]
1b:["$","div","sep-6",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
25:T5a6,The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.1c:["$","div","6",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://arxiv.org/pdf/2406.05946","target":"_blank","children":"Safety Alignment Should Be Made More Than Just a Few Tokens Deep"}]}],"$undefined",["$","div",null,{"className":"ArticlePreview_publishedTime__MauIG","children":["published: ","2024-06-09"]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","p","p-0",{"children":"$25"}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://arxiv.org/pdf/2406.05946","target":"_blank","children":"üîó"}]}]]}]
1d:["$","div","sep-7",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
26:T4ed,In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we re-examine the notion of "quality" from the perspective of pre- and post-training co-design. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.1e:["$","div","7",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://arxiv.org/pdf/2505.04741","target":"_blank","children":"When Bad Data Leads to Good Models"}]}],"$undefined",["$","div",null,{"className":"ArticlePreview_publishedTime__MauIG","children":["published: ","2025-05-07"]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","p","p-0",{"children":"$26"}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://arxiv.org/pdf/2505.04741","target":"_blank","children":"üîó"}]}]]}]
1f:["$","div","sep-8",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
20:["$","div","8",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://arxiv.org/pdf/2306.11644","target":"_blank","children":"Textbooks Are All You Need"}]}],"$undefined",["$","div",null,{"className":"ArticlePreview_publishedTime__MauIG","children":["published: ","2023-06-20"]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","p","p-0",{"children":"We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval."}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://arxiv.org/pdf/2306.11644","target":"_blank","children":"üîó"}]}]]}]
21:["$","div","sep-9",{"className":"page_addedDate__qyLFb","children":"2025-08-14"}]
22:["$","div","9",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf","target":"_blank","children":"Redirecting‚Ä¶"}]}],"$undefined","$undefined",["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"This paper details a randomized controlled trial (RCT) investigating the impact of early-2025 AI tools on the productivity of experienced open-source developers."}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Methodology:"}]," The study involved 16 developers with an average of 5 years of experience on specific, mature open-source projects. They completed 246 tasks, with each task randomly assigned to either allow or disallow the use of AI tools like Cursor Pro and Claude 3.5/3.7 Sonnet."]}],"\n",["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"Expectation vs. Reality:"}]," Developers predicted that using AI would decrease task completion time by 24% before the study and estimated a 20% time reduction after the study."]}],"\n",["$","li","li-3",{"children":[["$","strong","strong-0",{"children":"Core Finding:"}]," Contrary to expectations from both developers and experts, allowing AI tools ",["$","em","em-0",{"children":"increased"}]," task completion time by 19%."]}],"\n",["$","li","li-4",{"children":[["$","strong","strong-0",{"children":"Analysis:"}]," The authors investigated 20 different properties of the experimental setting that could explain this slowdown, but the effect remained robust across their analyses, suggesting it is not primarily an artifact of the experimental design."]}],"\n"]}]]}],["$","div",null,{"className":"ArticlePreview_footer__lkeLY","children":["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf","target":"_blank","children":"üîó"}]}]]}]
23:["$","div",null,{"className":"page_toolbar__VAh0U","children":[["$","div",null,{"className":"page_navigation___3IFr","children":[["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"/all/69","target":"$undefined","children":"newer"}],["$","$Lf",null,{"className":"LinkButton_LinkButton__nW1G0","href":"/all/71","target":"$undefined","children":"older"}]]}],["$","div",null,{"className":"page_location__WbWIR","children":[["$","span",null,{"className":"FocusSpan_FocusSpan__MPt_V","children":71}],"/",86]}]]}]
