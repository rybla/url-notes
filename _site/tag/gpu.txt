1:"$Sreact.fragment"
2:I[7555,[],""]
3:I[1295,[],""]
5:I[9665,[],"OutletBoundary"]
7:I[4911,[],"AsyncMetadataOutlet"]
9:I[9665,[],"ViewportBoundary"]
b:I[9665,[],"MetadataBoundary"]
c:"$Sreact.suspense"
e:I[8393,[],""]
:HL["/url-notes/_next/static/css/f03c873af434c7c6.css","style"]
:HL["/url-notes/_next/static/css/82198f0047596010.css","style"]
:HL["/url-notes/_next/static/css/542742ad04044919.css","style"]
0:{"P":null,"b":"RSyM0A8xOssA4iaMxna30","p":"/url-notes","c":["","tag","gpu"],"i":false,"f":[[["",{"children":["tag",{"children":[["tag","gpu","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/url-notes/_next/static/css/f03c873af434c7c6.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":["tag",["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["tag","gpu","d"],["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L4",[["$","link","0",{"rel":"stylesheet","href":"/url-notes/_next/static/css/82198f0047596010.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/url-notes/_next/static/css/542742ad04044919.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L5",null,{"children":["$L6",["$","$L7",null,{"promise":"$@8"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,[["$","$L9",null,{"children":"$La"}],null],["$","$Lb",null,{"children":["$","div",null,{"hidden":true,"children":["$","$c",null,{"fallback":null,"children":"$Ld"}]}]}]]}],false]],"m":"$undefined","G":["$e",[]],"s":false,"S":true}
f:I[6874,["874","static/chunks/874-437a265a67d6cfee.js","296","static/chunks/app/tag/%5Btag%5D/page-3ad24d2a8735be85.js"],""]
4:["$","div",null,{"className":"page_page___hQFP","children":[["$","div",null,{"className":"page_header__Y1ecl","children":"url-notes | tag | gpu"}],["$","div",null,{"className":"page_content__fAQW6","children":["$","div",null,{"className":"page_previews__SpAnf","children":[["$","div","0",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://news.ycombinator.com/item?id=44840728","target":"_blank","children":"Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?"}]}],["$","div",null,{"className":"ArticlePreview_tags__y8wnE","children":[["$","$Lf","0",{"className":"ArticlePreview_tag___oIyn","href":"/tag/llm","children":"llm"}],["$","$Lf","1",{"className":"ArticlePreview_tag___oIyn","href":"/tag/large%20language%20model","children":"large language model"}],["$","$Lf","2",{"className":"ArticlePreview_tag___oIyn","href":"/tag/chatgpt","children":"chatgpt"}],["$","$Lf","3",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gpt-4","children":"gpt-4"}],["$","$Lf","4",{"className":"ArticlePreview_tag___oIyn","href":"/tag/inference","children":"inference"}],["$","$Lf","5",{"className":"ArticlePreview_tag___oIyn","href":"/tag/scalability","children":"scalability"}],["$","$Lf","6",{"className":"ArticlePreview_tag___oIyn","href":"/tag/hardware","children":"hardware"}],["$","$Lf","7",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gpu","children":"gpu"}],["$","$Lf","8",{"className":"ArticlePreview_tag___oIyn","href":"/tag/tpu","children":"tpu"}],["$","$Lf","9",{"className":"ArticlePreview_tag___oIyn","href":"/tag/parallelism","children":"parallelism"}],["$","$Lf","10",{"className":"ArticlePreview_tag___oIyn","href":"/tag/batching","children":"batching"}],["$","$Lf","11",{"className":"ArticlePreview_tag___oIyn","href":"/tag/memory%20bandwidth","children":"memory bandwidth"}],["$","$Lf","12",{"className":"ArticlePreview_tag___oIyn","href":"/tag/model%20optimization","children":"model optimization"}],["$","$Lf","13",{"className":"ArticlePreview_tag___oIyn","href":"/tag/mixture%20of%20experts","children":"mixture of experts"}],["$","$Lf","14",{"className":"ArticlePreview_tag___oIyn","href":"/tag/moe","children":"moe"}],["$","$Lf","15",{"className":"ArticlePreview_tag___oIyn","href":"/tag/quantization","children":"quantization"}],["$","$Lf","16",{"className":"ArticlePreview_tag___oIyn","href":"/tag/speculative%20decoding","children":"speculative decoding"}],["$","$Lf","17",{"className":"ArticlePreview_tag___oIyn","href":"/tag/kv%20cache","children":"kv cache"}],["$","$Lf","18",{"className":"ArticlePreview_tag___oIyn","href":"/tag/economics","children":"economics"}],["$","$Lf","19",{"className":"ArticlePreview_tag___oIyn","href":"/tag/infrastructure","children":"infrastructure"}],["$","$Lf","20",{"className":"ArticlePreview_tag___oIyn","href":"/tag/hacker%20news","children":"hacker news"}]]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":["\n",["$","p","p-0",{"children":[["$","strong","strong-0",{"children":"Batching & Parallelism"}],":"]}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Large-scale inference relies on batching, where multiple user requests are processed simultaneously."}],"\n",["$","li","li-1",{"children":"The primary bottleneck in LLM inference is memory bandwidth (loading model weights from VRAM to compute units)."}],"\n",["$","li","li-2",{"children":"By batching requests, weights are loaded once and applied to many queries, amortizing the high cost of memory access and dramatically increasing throughput."}],"\n",["$","li","li-3",{"children":"This process is highly parallelizable across large clusters of GPUs."}],"\n"]}],"\n"]}],"\n",["$","li","li-1",{"children":["\n",["$","p","p-0",{"children":[["$","strong","strong-0",{"children":"Hardware & Infrastructure"}],":"]}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Massive investment in specialized hardware like NVIDIA H100s or Google TPUs, which feature large amounts of high-bandwidth memory (HBM)."}],"\n",["$","li","li-1",{"children":"Models are sharded (split) across multiple GPUs (tensor and pipeline parallelism), so a single model runs on a cluster of interconnected accelerators."}],"\n","$L10","\n"]}],"\n"]}],"\n","$L11","\n","$L12","\n"]}]]}]]}],"$L13","$L14"]}]}]]}]
10:["$","li","li-2",{"children":"The infrastructure is built for high utilization, unlike a local setup which is mostly idle."}]
11:["$","li","li-2",{"children":["\n",["$","p","p-0",{"children":[["$","strong","strong-0",{"children":"Model Optimizations"}],":"]}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"Mixture of Experts (MoE)"}],": Models are designed so that only a fraction of the total parameters (the \"experts\") are activated for any given token, reducing the computational cost per inference."]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Quantization"}],": Model weights are compressed to lower-precision formats (e.g., INT8, FP8) to reduce memory footprint and bandwidth requirements."]}],"\n",["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"Speculative Decoding"}],": A smaller, faster \"draft\" model generates candidate tokens which are then validated in parallel by the larger, more powerful model, increasing token generation speed."]}],"\n",["$","li","li-3",{"children":[["$","strong","strong-0",{"children":"KV Cache"}],": The key-value state of the attention mechanism is cached, so the context doesn't need to be re-processed for each new token generated in a sequence."]}],"\n"]}],"\n"]}]
12:["$","li","li-3",{"children":["\n",["$","p","p-0",{"children":[["$","strong","strong-0",{"children":"Economic Model"}],":"]}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"The operation is funded by billions of dollars in investment, allowing companies to run at a loss to capture market share."}],"\n",["$","li","li-1",{"children":"The cost per user is manageable because of the efficiencies of scale and because the vast majority of users are idle most of the time, allowing for high multi-tenancy on the hardware."}],"\n"]}],"\n"]}]
13:["$","div","1",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://world.hey.com/dhh/the-framework-desktop-is-a-beast-636fb4ff","target":"_blank","children":"The Framework Desktop is a beast"}]}],["$","div",null,{"className":"ArticlePreview_tags__y8wnE","children":[["$","$Lf","0",{"className":"ArticlePreview_tag___oIyn","href":"/tag/hardware","children":"hardware"}],["$","$Lf","1",{"className":"ArticlePreview_tag___oIyn","href":"/tag/desktop%20computer","children":"desktop computer"}],["$","$Lf","2",{"className":"ArticlePreview_tag___oIyn","href":"/tag/mini%20pc","children":"mini pc"}],["$","$Lf","3",{"className":"ArticlePreview_tag___oIyn","href":"/tag/framework","children":"framework"}],["$","$Lf","4",{"className":"ArticlePreview_tag___oIyn","href":"/tag/framework%20desktop","children":"framework desktop"}],["$","$Lf","5",{"className":"ArticlePreview_tag___oIyn","href":"/tag/amd","children":"amd"}],["$","$Lf","6",{"className":"ArticlePreview_tag___oIyn","href":"/tag/ryzen","children":"ryzen"}],["$","$Lf","7",{"className":"ArticlePreview_tag___oIyn","href":"/tag/apple","children":"apple"}],["$","$Lf","8",{"className":"ArticlePreview_tag___oIyn","href":"/tag/m4","children":"m4"}],["$","$Lf","9",{"className":"ArticlePreview_tag___oIyn","href":"/tag/mac%20studio","children":"mac studio"}],["$","$Lf","10",{"className":"ArticlePreview_tag___oIyn","href":"/tag/intel","children":"intel"}],["$","$Lf","11",{"className":"ArticlePreview_tag___oIyn","href":"/tag/nvidia","children":"nvidia"}],["$","$Lf","12",{"className":"ArticlePreview_tag___oIyn","href":"/tag/rtx%204060","children":"rtx 4060"}],["$","$Lf","13",{"className":"ArticlePreview_tag___oIyn","href":"/tag/cpu","children":"cpu"}],["$","$Lf","14",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gpu","children":"gpu"}],["$","$Lf","15",{"className":"ArticlePreview_tag___oIyn","href":"/tag/unified%20memory","children":"unified memory"}],["$","$Lf","16",{"className":"ArticlePreview_tag___oIyn","href":"/tag/small%20form%20factor","children":"small form factor"}],["$","$Lf","17",{"className":"ArticlePreview_tag___oIyn","href":"/tag/sff","children":"sff"}],["$","$Lf","18",{"className":"ArticlePreview_tag___oIyn","href":"/tag/performance","children":"performance"}],["$","$Lf","19",{"className":"ArticlePreview_tag___oIyn","href":"/tag/benchmark","children":"benchmark"}],["$","$Lf","20",{"className":"ArticlePreview_tag___oIyn","href":"/tag/geekbench","children":"geekbench"}],["$","$Lf","21",{"className":"ArticlePreview_tag___oIyn","href":"/tag/development","children":"development"}],["$","$Lf","22",{"className":"ArticlePreview_tag___oIyn","href":"/tag/docker","children":"docker"}],["$","$Lf","23",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gaming","children":"gaming"}],["$","$Lf","24",{"className":"ArticlePreview_tag___oIyn","href":"/tag/ai","children":"ai"}],["$","$Lf","25",{"className":"ArticlePreview_tag___oIyn","href":"/tag/large%20language%20models","children":"large language models"}],["$","$Lf","26",{"className":"ArticlePreview_tag___oIyn","href":"/tag/llm","children":"llm"}],["$","$Lf","27",{"className":"ArticlePreview_tag___oIyn","href":"/tag/review","children":"review"}],["$","$Lf","28",{"className":"ArticlePreview_tag___oIyn","href":"/tag/value","children":"value"}],["$","$Lf","29",{"className":"ArticlePreview_tag___oIyn","href":"/tag/pricing","children":"pricing"}]]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"CPU:"}]," AMD Ryzen AI Max 395+ (16 Zen5 cores @ 5.1GHz), a laptop-class processor."]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Form Factor:"}]," 4.5L volume, noted for being quiet even under full load."]}],"\n",["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"Multi-Core Performance:"}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Outperforms Apple M4 Max, M4 Pro, and Intel 14900K in Geekbench 6 multi-core benchmarks."}],"\n",["$","li","li-1",{"children":"Excels in Docker-based development workflows (e.g., Ruby test suite with MySQL/Redis/ElasticSearch), showing a ~40% speed increase over an M4 Max, partially due to Docker's native performance on Linux."}],"\n"]}],"\n"]}],"\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n"]}]]}]]}]
14:["$","div","2",{"className":"ArticlePreview_ArticlePreview__59E_4","children":[["$","div",null,{"className":"ArticlePreview_title__Snpua","children":["$","$Lf",null,{"href":"https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the","target":"_blank","children":"From GPT-2 to gpt-oss: Analyzing the Architectural Advances"}]}],["$","div",null,{"className":"ArticlePreview_tags__y8wnE","children":[["$","$Lf","0",{"className":"ArticlePreview_tag___oIyn","href":"/tag/large%20language%20model","children":"large language model"}],["$","$Lf","1",{"className":"ArticlePreview_tag___oIyn","href":"/tag/llm","children":"llm"}],["$","$Lf","2",{"className":"ArticlePreview_tag___oIyn","href":"/tag/model%20architecture","children":"model architecture"}],["$","$Lf","3",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gpt-oss","children":"gpt-oss"}],["$","$Lf","4",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gpt-2","children":"gpt-2"}],["$","$Lf","5",{"className":"ArticlePreview_tag___oIyn","href":"/tag/mixture-of-experts","children":"mixture-of-experts"}],["$","$Lf","6",{"className":"ArticlePreview_tag___oIyn","href":"/tag/moe","children":"moe"}],["$","$Lf","7",{"className":"ArticlePreview_tag___oIyn","href":"/tag/rmsnorm","children":"rmsnorm"}],["$","$Lf","8",{"className":"ArticlePreview_tag___oIyn","href":"/tag/swiglu","children":"swiglu"}],["$","$Lf","9",{"className":"ArticlePreview_tag___oIyn","href":"/tag/rotary%20position%20embeddings","children":"rotary position embeddings"}],["$","$Lf","10",{"className":"ArticlePreview_tag___oIyn","href":"/tag/rope","children":"rope"}],["$","$Lf","11",{"className":"ArticlePreview_tag___oIyn","href":"/tag/grouped%20query%20attention","children":"grouped query attention"}],["$","$Lf","12",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gqa","children":"gqa"}],["$","$Lf","13",{"className":"ArticlePreview_tag___oIyn","href":"/tag/sliding-window%20attention","children":"sliding-window attention"}],["$","$Lf","14",{"className":"ArticlePreview_tag___oIyn","href":"/tag/quantization","children":"quantization"}],["$","$Lf","15",{"className":"ArticlePreview_tag___oIyn","href":"/tag/reinforcement%20learning","children":"reinforcement learning"}],["$","$Lf","16",{"className":"ArticlePreview_tag___oIyn","href":"/tag/open%20source","children":"open source"}],["$","$Lf","17",{"className":"ArticlePreview_tag___oIyn","href":"/tag/apache%202.0","children":"apache 2.0"}],["$","$Lf","18",{"className":"ArticlePreview_tag___oIyn","href":"/tag/ai","children":"ai"}],["$","$Lf","19",{"className":"ArticlePreview_tag___oIyn","href":"/tag/deep%20learning","children":"deep learning"}],["$","$Lf","20",{"className":"ArticlePreview_tag___oIyn","href":"/tag/technical%20analysis","children":"technical analysis"}],["$","$Lf","21",{"className":"ArticlePreview_tag___oIyn","href":"/tag/gpu","children":"gpu"}]]}],["$","div",null,{"className":"ArticlePreview_summary__Zyb4E","children":[["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":["\n",["$","p","p-0",{"children":["$","strong","strong-0",{"children":"Architectural Evolution from GPT-2:"}]}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"Normalization:"}]," Replaced LayerNorm with the computationally simpler RMSNorm."]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Activation:"}]," Switched from GELU to a gated SwiGLU (Swish Gated Linear Unit), offering better expressivity with fewer parameters."]}],"\n",["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"Positional Encoding:"}]," Upgraded from learned absolute positional embeddings to Rotary Position Embeddings (RoPE)."]}],"\n",["$","li","li-3",{"children":[["$","strong","strong-0",{"children":"Attention:"}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Uses Grouped Query Attention (GQA) for improved efficiency over Multi-Head Attention (MHA)."}],"\n",["$","li","li-1",{"children":"Implements sliding-window attention (128-token window) in every other layer, a technique reportedly used in GPT-3."}],"\n",["$","li","li-2",{"children":"Re-introduces bias units in attention projections and adds learned \"attention sinks\" (per-head bias logits) to stabilize attention over long contexts."}],"\n"]}],"\n"]}],"\n","$L19","\n"]}],"\n"]}],"\n","$L1a","\n","$L1b","\n"]}]]}]]}]
15:["$","li","li-3",{"children":[["$","strong","strong-0",{"children":"Single-Core Performance:"}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Approximately 20% slower than Apple's M4 series in single-core tasks."}],"\n",["$","li","li-1",{"children":"Speedometer 2.1 benchmark: 670 (vs. 744 on M4 Pro)."}],"\n"]}],"\n"]}]
16:["$","li","li-4",{"children":[["$","strong","strong-0",{"children":"Memory:"}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Utilizes unified memory, configurable up to 128GB."}],"\n",["$","li","li-1",{"children":"Suitable for running large local LLMs (e.g., 120b models at ~40 tokens/second)."}],"\n"]}],"\n"]}]
17:["$","li","li-5",{"children":[["$","strong","strong-0",{"children":"Graphics:"}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"Integrated GPU performance is comparable to a discrete NVIDIA RTX 4060."}],"\n",["$","li","li-1",{"children":"Capable of running modern games at 1440p on high settings."}],"\n"]}],"\n"]}]
18:["$","li","li-6",{"children":[["$","strong","strong-0",{"children":"Value:"}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"A 64GB RAM / 2TB NVMe configuration is priced at $1,876, nearly half the cost of a similarly specced Mac Studio ($3,299)."}],"\n",["$","li","li-1",{"children":"Positioned as a higher-performance, more expensive alternative to other mini PCs like the Beelink SER9."}],"\n"]}],"\n"]}]
19:["$","li","li-4",{"children":[["$","strong","strong-0",{"children":"Regularization:"}]," Removed dropout, which is common in modern LLMs trained on massive datasets for a single epoch."]}]
1a:["$","li","li-1",{"children":["\n",["$","p","p-0",{"children":["$","strong","strong-0",{"children":"Core Architecture:"}]}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"Mixture-of-Experts (MoE):"}]," Replaces standard feed-forward networks with MoE layers. The ",["$","code","code-0",{"children":"gpt-oss-20b"}]," model uses 32 experts and activates 4 per token."]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Width vs. Depth:"}]," Compared to the contemporary Qwen3 model, ",["$","code","code-0",{"children":"gpt-oss"}]," is a \"wider\" architecture (larger embedding dimension) rather than \"deeper\" (fewer transformer blocks)."]}],"\n"]}],"\n"]}]
1b:["$","li","li-2",{"children":["\n",["$","p","p-0",{"children":["$","strong","strong-0",{"children":"Training and Features:"}]}],"\n",["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":[["$","strong","strong-0",{"children":"Reasoning Control:"}]," Trained with a high-compute reinforcement learning stage that allows users to control reasoning verbosity and accuracy at inference time via a system prompt (",["$","code","code-0",{"children":"Reasoning effort: low/medium/high"}],")."]}],"\n",["$","li","li-1",{"children":[["$","strong","strong-0",{"children":"Quantization:"}]," Released with an MXFP4 quantization scheme for MoE experts, enabling the 120B model to run on a single 80GB H100 GPU and the 20B model on a 16GB RTX 50-series GPU."]}],"\n",["$","li","li-2",{"children":[["$","strong","strong-0",{"children":"License:"}]," Apache 2.0 open-weight license."]}],"\n"]}],"\n"]}]
a:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
6:null
8:{"metadata":[["$","title","0",{"children":"url-notes | tag | gpu"}]],"error":null,"digest":"$undefined"}
d:"$8:metadata"
