<!DOCTYPE html><!--UE_zlTBY9TySNjoJB9Enp--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/url-notes/_next/static/css/f03c873af434c7c6.css" data-precedence="next"/><link rel="stylesheet" href="/url-notes/_next/static/css/0e5ea1ea0183b412.css" data-precedence="next"/><link rel="stylesheet" href="/url-notes/_next/static/css/7190d9c623ab1fe0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/url-notes/_next/static/chunks/webpack-54d9d4b2aa649176.js"/><script src="/url-notes/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/url-notes/_next/static/chunks/964-a29425d4972030f1.js" async=""></script><script src="/url-notes/_next/static/chunks/main-app-e4d4697bcd6cfe75.js" async=""></script><script src="/url-notes/_next/static/chunks/874-437a265a67d6cfee.js" async=""></script><script src="/url-notes/_next/static/chunks/app/tag/%5Btag%5D/page-abef1952a2d9d9ae.js" async=""></script><title>url-notes | tag | gpu</title><link rel="icon" href="/url-notes/favicon.ico" type="image/x-icon" sizes="256x256"/><script src="/url-notes/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><div><div class="Header_Header__VDN4T"><div class="Header_item__PWJos"><a href="/url-notes">url-notes</a></div><div class="Header_separator__zF22U">|</div><div class="Header_item__PWJos"><a href="/url-notes/tags">tag</a></div><div class="Header_separator__zF22U">|</div><div class="Header_item__PWJos"><a href="/url-notes/tags">tag</a></div><div class="Header_separator__zF22U">|</div><div class="Header_item__PWJos"><span>gpu</span></div></div><div class="page_content__fAQW6"><div class="page_previews__SpAnf"><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://code.ffmpeg.org/FFmpeg/FFmpeg/commit/13ce36fef98a3f4e6d8360c24d6b8434cbb8869b">libavfilter: Whisper audio filter · 13ce36fef9</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/speech-to-text">speech-to-text</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/transcription">transcription</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/audio%20processing">audio processing</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ffmpeg">ffmpeg</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/libavfilter">libavfilter</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/whisper">whisper</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/whisper.cpp">whisper.cpp</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/voice%20activity%20detection">voice activity detection</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/vad">vad</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/hardware%20acceleration">hardware acceleration</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpu">gpu</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/metadata">metadata</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/multimedia">multimedia</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/open%20source">open source</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/srt">srt</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/json">json</a></div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li><strong>New Feature:</strong> Adds a new <code>whisper</code> audio filter to FFmpeg&#x27;s <code>libavfilter</code> for speech-to-text transcription.</li>
<li><strong>Core Technology:</strong> Integrates the <code>whisper.cpp</code> library to perform the actual audio processing.</li>
<li><strong>Processing Model:</strong>
<ul>
<li>Buffers incoming floating-point audio samples at the required <code>WHISPER_SAMPLE_RATE</code>.</li>
<li>Executes transcription on the buffered audio via the <code>whisper_full</code> function.</li>
<li>The transcription result is attached as metadata (<code>lavfi.whisper.text</code>) to the output <code>AVFrame</code>.</li>
</ul>
</li>
<li><strong>Voice Activity Detection (VAD):</strong>
<ul>
<li>Optionally uses a separate VAD model to detect speech segments.</li>
<li>This allows the filter to trigger transcription on meaningful chunks of speech rather than on fixed-size blocks of audio.</li>
<li>VAD behavior is configurable with parameters like <code>vad_threshold</code> and <code>vad_min_silence_duration</code>.</li>
</ul>
</li>
<li><strong>Hardware Acceleration:</strong> Supports offloading computation to a GPU through the <code>use_gpu</code> and <code>gpu_device</code> options.</li>
<li><strong>Output Formatting:</strong> Can output transcription results to a file or pipe (<code>destination</code> option) in multiple formats, including <code>srt</code>, <code>json</code>, or plain <code>text</code>.</li>
</ul></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://news.ycombinator.com/item?id=44840728">Ask HN: How can ChatGPT serve 700M users when I can&#x27;t run one GPT-4 locally?</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/llm">llm</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/large%20language%20model">large language model</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/chatgpt">chatgpt</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpt-4">gpt-4</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/inference">inference</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/scalability">scalability</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/hardware">hardware</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpu">gpu</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/tpu">tpu</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/parallelism">parallelism</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/batching">batching</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/memory%20bandwidth">memory bandwidth</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/model%20optimization">model optimization</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/mixture%20of%20experts">mixture of experts</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/moe">moe</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/quantization">quantization</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/speculative%20decoding">speculative decoding</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/kv%20cache">kv cache</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/economics">economics</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/infrastructure">infrastructure</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/hacker%20news">hacker news</a></div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li>
<p><strong>Batching &amp; Parallelism</strong>:</p>
<ul>
<li>Large-scale inference relies on batching, where multiple user requests are processed simultaneously.</li>
<li>The primary bottleneck in LLM inference is memory bandwidth (loading model weights from VRAM to compute units).</li>
<li>By batching requests, weights are loaded once and applied to many queries, amortizing the high cost of memory access and dramatically increasing throughput.</li>
<li>This process is highly parallelizable across large clusters of GPUs.</li>
</ul>
</li>
<li>
<p><strong>Hardware &amp; Infrastructure</strong>:</p>
<ul>
<li>Massive investment in specialized hardware like NVIDIA H100s or Google TPUs, which feature large amounts of high-bandwidth memory (HBM).</li>
<li>Models are sharded (split) across multiple GPUs (tensor and pipeline parallelism), so a single model runs on a cluster of interconnected accelerators.</li>
<li>The infrastructure is built for high utilization, unlike a local setup which is mostly idle.</li>
</ul>
</li>
<li>
<p><strong>Model Optimizations</strong>:</p>
<ul>
<li><strong>Mixture of Experts (MoE)</strong>: Models are designed so that only a fraction of the total parameters (the &quot;experts&quot;) are activated for any given token, reducing the computational cost per inference.</li>
<li><strong>Quantization</strong>: Model weights are compressed to lower-precision formats (e.g., INT8, FP8) to reduce memory footprint and bandwidth requirements.</li>
<li><strong>Speculative Decoding</strong>: A smaller, faster &quot;draft&quot; model generates candidate tokens which are then validated in parallel by the larger, more powerful model, increasing token generation speed.</li>
<li><strong>KV Cache</strong>: The key-value state of the attention mechanism is cached, so the context doesn&#x27;t need to be re-processed for each new token generated in a sequence.</li>
</ul>
</li>
<li>
<p><strong>Economic Model</strong>:</p>
<ul>
<li>The operation is funded by billions of dollars in investment, allowing companies to run at a loss to capture market share.</li>
<li>The cost per user is manageable because of the efficiencies of scale and because the vast majority of users are idle most of the time, allowing for high multi-tenancy on the hardware.</li>
</ul>
</li>
</ul></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://world.hey.com/dhh/the-framework-desktop-is-a-beast-636fb4ff">The Framework Desktop is a beast</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/hardware">hardware</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/desktop%20computer">desktop computer</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/mini%20pc">mini pc</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/framework">framework</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/framework%20desktop">framework desktop</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/amd">amd</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ryzen">ryzen</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/apple">apple</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/m4">m4</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/mac%20studio">mac studio</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/intel">intel</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/nvidia">nvidia</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/rtx%204060">rtx 4060</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/cpu">cpu</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpu">gpu</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/unified%20memory">unified memory</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/small%20form%20factor">small form factor</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/sff">sff</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/performance">performance</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/benchmark">benchmark</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/geekbench">geekbench</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/development">development</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/docker">docker</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gaming">gaming</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ai">ai</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/large%20language%20models">large language models</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/llm">llm</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/review">review</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/value">value</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/pricing">pricing</a></div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li><strong>CPU:</strong> AMD Ryzen AI Max 395+ (16 Zen5 cores @ 5.1GHz), a laptop-class processor.</li>
<li><strong>Form Factor:</strong> 4.5L volume, noted for being quiet even under full load.</li>
<li><strong>Multi-Core Performance:</strong>
<ul>
<li>Outperforms Apple M4 Max, M4 Pro, and Intel 14900K in Geekbench 6 multi-core benchmarks.</li>
<li>Excels in Docker-based development workflows (e.g., Ruby test suite with MySQL/Redis/ElasticSearch), showing a ~40% speed increase over an M4 Max, partially due to Docker&#x27;s native performance on Linux.</li>
</ul>
</li>
<li><strong>Single-Core Performance:</strong>
<ul>
<li>Approximately 20% slower than Apple&#x27;s M4 series in single-core tasks.</li>
<li>Speedometer 2.1 benchmark: 670 (vs. 744 on M4 Pro).</li>
</ul>
</li>
<li><strong>Memory:</strong>
<ul>
<li>Utilizes unified memory, configurable up to 128GB.</li>
<li>Suitable for running large local LLMs (e.g., 120b models at ~40 tokens/second).</li>
</ul>
</li>
<li><strong>Graphics:</strong>
<ul>
<li>Integrated GPU performance is comparable to a discrete NVIDIA RTX 4060.</li>
<li>Capable of running modern games at 1440p on high settings.</li>
</ul>
</li>
<li><strong>Value:</strong>
<ul>
<li>A 64GB RAM / 2TB NVMe configuration is priced at $1,876, nearly half the cost of a similarly specced Mac Studio ($3,299).</li>
<li>Positioned as a higher-performance, more expensive alternative to other mini PCs like the Beelink SER9.</li>
</ul>
</li>
</ul></div></div><div class="ArticlePreview_ArticlePreview__59E_4"><div class="ArticlePreview_title__Snpua"><a target="_blank" href="https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the">From GPT-2 to gpt-oss: Analyzing the Architectural Advances</a></div><div class="ArticlePreview_tags__y8wnE"><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/large%20language%20model">large language model</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/llm">llm</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/model%20architecture">model architecture</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpt-oss">gpt-oss</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpt-2">gpt-2</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/mixture-of-experts">mixture-of-experts</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/moe">moe</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/rmsnorm">rmsnorm</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/swiglu">swiglu</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/rotary%20position%20embeddings">rotary position embeddings</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/rope">rope</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/grouped%20query%20attention">grouped query attention</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gqa">gqa</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/sliding-window%20attention">sliding-window attention</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/quantization">quantization</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/reinforcement%20learning">reinforcement learning</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/open%20source">open source</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/apache%202.0">apache 2.0</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/ai">ai</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/deep%20learning">deep learning</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/technical%20analysis">technical analysis</a><a class="ArticlePreview_tag___oIyn" href="/url-notes/tag/gpu">gpu</a></div><div class="ArticlePreview_summary__Zyb4E"><ul>
<li>
<p><strong>Architectural Evolution from GPT-2:</strong></p>
<ul>
<li><strong>Normalization:</strong> Replaced LayerNorm with the computationally simpler RMSNorm.</li>
<li><strong>Activation:</strong> Switched from GELU to a gated SwiGLU (Swish Gated Linear Unit), offering better expressivity with fewer parameters.</li>
<li><strong>Positional Encoding:</strong> Upgraded from learned absolute positional embeddings to Rotary Position Embeddings (RoPE).</li>
<li><strong>Attention:</strong>
<ul>
<li>Uses Grouped Query Attention (GQA) for improved efficiency over Multi-Head Attention (MHA).</li>
<li>Implements sliding-window attention (128-token window) in every other layer, a technique reportedly used in GPT-3.</li>
<li>Re-introduces bias units in attention projections and adds learned &quot;attention sinks&quot; (per-head bias logits) to stabilize attention over long contexts.</li>
</ul>
</li>
<li><strong>Regularization:</strong> Removed dropout, which is common in modern LLMs trained on massive datasets for a single epoch.</li>
</ul>
</li>
<li>
<p><strong>Core Architecture:</strong></p>
<ul>
<li><strong>Mixture-of-Experts (MoE):</strong> Replaces standard feed-forward networks with MoE layers. The <code>gpt-oss-20b</code> model uses 32 experts and activates 4 per token.</li>
<li><strong>Width vs. Depth:</strong> Compared to the contemporary Qwen3 model, <code>gpt-oss</code> is a &quot;wider&quot; architecture (larger embedding dimension) rather than &quot;deeper&quot; (fewer transformer blocks).</li>
</ul>
</li>
<li>
<p><strong>Training and Features:</strong></p>
<ul>
<li><strong>Reasoning Control:</strong> Trained with a high-compute reinforcement learning stage that allows users to control reasoning verbosity and accuracy at inference time via a system prompt (<code>Reasoning effort: low/medium/high</code>).</li>
<li><strong>Quantization:</strong> Released with an MXFP4 quantization scheme for MoE experts, enabling the 120B model to run on a single 80GB H100 GPU and the 20B model on a 16GB RTX 50-series GPU.</li>
<li><strong>License:</strong> Apache 2.0 open-weight license.</li>
</ul>
</li>
</ul></div></div></div></div></div><!--$--><!--/$--><script src="/url-notes/_next/static/chunks/webpack-54d9d4b2aa649176.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7555,[],\"\"]\n3:I[1295,[],\"\"]\n5:I[9665,[],\"OutletBoundary\"]\n7:I[4911,[],\"AsyncMetadataOutlet\"]\n9:I[9665,[],\"ViewportBoundary\"]\nb:I[9665,[],\"MetadataBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[8393,[],\"\"]\n:HL[\"/url-notes/_next/static/css/f03c873af434c7c6.css\",\"style\"]\n:HL[\"/url-notes/_next/static/css/0e5ea1ea0183b412.css\",\"style\"]\n:HL[\"/url-notes/_next/static/css/7190d9c623ab1fe0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"UE-zlTBY9TySNjoJB9Enp\",\"p\":\"/url-notes\",\"c\":[\"\",\"tag\",\"gpu\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"tag\",{\"children\":[[\"tag\",\"gpu\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/f03c873af434c7c6.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[\"tag\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"tag\",\"gpu\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/0e5ea1ea0183b412.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/url-notes/_next/static/css/7190d9c623ab1fe0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$L6\",[\"$\",\"$L7\",null,{\"promise\":\"$@8\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$c\",null,{\"fallback\":null,\"children\":\"$Ld\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"f:I[8175,[],\"IconMark\"]\n8:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"url-notes | tag | gpu\"}],[\"$\",\"link\",\"1\",{\"rel\":\"icon\",\"href\":\"/url-notes/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"256x256\"}],[\"$\",\"$Lf\",\"2\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"d:\"$8:metadata\"\n"])</script><script>self.__next_f.push([1,"10:I[6874,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"296\",\"static/chunks/app/tag/%5Btag%5D/page-abef1952a2d9d9ae.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"$undefined\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Header_Header__VDN4T\",\"children\":[[\"$\",\"div\",null,{\"className\":\"Header_item__PWJos\",\"children\":[\"$\",\"$L10\",null,{\"href\":\"/\",\"children\":\"url-notes\"}]}],[[[\"$\",\"div\",\"separator-0\",{\"className\":\"Header_separator__zF22U\",\"children\":\"|\"}],[\"$\",\"div\",\"item-0\",{\"className\":\"Header_item__PWJos\",\"children\":[\"$\",\"$L10\",\"0\",{\"href\":\"/tags\",\"children\":\"tag\"}]}]],[[\"$\",\"div\",\"separator-1\",{\"className\":\"Header_separator__zF22U\",\"children\":\"|\"}],[\"$\",\"div\",\"item-1\",{\"className\":\"Header_item__PWJos\",\"children\":[\"$\",\"$L10\",\"1\",{\"href\":\"/tags\",\"children\":\"tag\"}]}]],[[\"$\",\"div\",\"separator-2\",{\"className\":\"Header_separator__zF22U\",\"children\":\"|\"}],[\"$\",\"div\",\"item-2\",{\"className\":\"Header_item__PWJos\",\"children\":[\"$\",\"span\",\"2\",{\"children\":\"gpu\"}]}]]]]}],[\"$\",\"div\",null,{\"className\":\"page_content__fAQW6\",\"children\":[\"$\",\"div\",null,{\"className\":\"page_previews__SpAnf\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$L10\",null,{\"href\":\"https://code.ffmpeg.org/FFmpeg/FFmpeg/commit/13ce36fef98a3f4e6d8360c24d6b8434cbb8869b\",\"target\":\"_blank\",\"children\":\"libavfilter: Whisper audio filter · 13ce36fef9\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$L10\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/speech-to-text\",\"children\":\"speech-to-text\"}],[\"$\",\"$L10\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/transcription\",\"children\":\"transcription\"}],[\"$\",\"$L10\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/audio%20processing\",\"children\":\"audio processing\"}],[\"$\",\"$L10\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ffmpeg\",\"children\":\"ffmpeg\"}],[\"$\",\"$L10\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/libavfilter\",\"children\":\"libavfilter\"}],[\"$\",\"$L10\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/whisper\",\"children\":\"whisper\"}],[\"$\",\"$L10\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/whisper.cpp\",\"children\":\"whisper.cpp\"}],[\"$\",\"$L10\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/voice%20activity%20detection\",\"children\":\"voice activity detection\"}],[\"$\",\"$L10\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/vad\",\"children\":\"vad\"}],[\"$\",\"$L10\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/hardware%20acceleration\",\"children\":\"hardware acceleration\"}],[\"$\",\"$L10\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpu\",\"children\":\"gpu\"}],[\"$\",\"$L10\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/metadata\",\"children\":\"metadata\"}],[\"$\",\"$L10\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/multimedia\",\"children\":\"multimedia\"}],[\"$\",\"$L10\",\"13\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/open%20source\",\"children\":\"open source\"}],[\"$\",\"$L10\",\"14\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/srt\",\"children\":\"srt\"}],[\"$\",\"$L10\",\"15\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/json\",\"children\":\"json\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"New Feature:\"}],\" Adds a new \",[\"$\",\"code\",\"code-0\",{\"children\":\"whisper\"}],\" audio filter to FFmpeg's \",[\"$\",\"code\",\"code-1\",{\"children\":\"libavfilter\"}],\" for speech-to-text transcription.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Core Technology:\"}],\" Integrates the \",[\"$\",\"code\",\"code-0\",{\"children\":\"whisper.cpp\"}],\" library to perform the actual audio processing.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Processing Model:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"Buffers incoming floating-point audio samples at the required \",[\"$\",\"code\",\"code-0\",{\"children\":\"WHISPER_SAMPLE_RATE\"}],\".\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"Executes transcription on the buffered audio via the \",[\"$\",\"code\",\"code-0\",{\"children\":\"whisper_full\"}],\" function.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"The transcription result is attached as metadata (\",[\"$\",\"code\",\"code-0\",{\"children\":\"lavfi.whisper.text\"}],\") to the output \",[\"$\",\"code\",\"code-1\",{\"children\":\"AVFrame\"}],\".\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Voice Activity Detection (VAD):\"}],\"\\n\",\"$L11\",\"\\n\"]}],\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\"]}]]}]]}],\"$L14\",\"$L15\",\"$L16\"]}]}]]}]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Optionally uses a separate VAD model to detect speech segments.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"This allows the filter to trigger transcription on meaningful chunks of speech rather than on fixed-size blocks of audio.\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"VAD behavior is configurable with parameters like \",[\"$\",\"code\",\"code-0\",{\"children\":\"vad_threshold\"}],\" and \",[\"$\",\"code\",\"code-1\",{\"children\":\"vad_min_silence_duration\"}],\".\"]}],\"\\n\"]}]\n12:[\"$\",\"li\",\"li-4\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Hardware Acceleration:\"}],\" Supports offloading computation to a GPU through the \",[\"$\",\"code\",\"code-0\",{\"children\":\"use_gpu\"}],\" and \",[\"$\",\"code\",\"code-1\",{\"children\":\"gpu_device\"}],\" options.\"]}]\n13:[\"$\",\"li\",\"li-5\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Output Formatting:\"}],\" Can output transcription results to a file or pipe (\",[\"$\",\"code\",\"code-0\",{\"children\":\"destination\"}],\" option) in multiple formats, including \",[\"$\",\"code\",\"code-1\",{\"children\":\"srt\"}],\", \",[\"$\",\"code\",\"code-2\",{\"children\":\"json\"}],\", or plain \",[\"$\",\"code\",\"code-3\",{\"children\":\"text\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",\"1\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$L10\",null,{\"href\":\"https://news.ycombinator.com/item?id=44840728\",\"target\":\"_blank\",\"children\":\"Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$L10\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/llm\",\"children\":\"llm\"}],[\"$\",\"$L10\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/large%20language%20model\",\"children\":\"large language model\"}],[\"$\",\"$L10\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/chatgpt\",\"children\":\"chatgpt\"}],[\"$\",\"$L10\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpt-4\",\"children\":\"gpt-4\"}],[\"$\",\"$L10\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/inference\",\"children\":\"inference\"}],[\"$\",\"$L10\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/scalability\",\"children\":\"scalability\"}],[\"$\",\"$L10\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/hardware\",\"children\":\"hardware\"}],[\"$\",\"$L10\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpu\",\"children\":\"gpu\"}],[\"$\",\"$L10\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/tpu\",\"children\":\"tpu\"}],[\"$\",\"$L10\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/parallelism\",\"children\":\"parallelism\"}],[\"$\",\"$L10\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/batching\",\"children\":\"batching\"}],[\"$\",\"$L10\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/memory%20bandwidth\",\"children\":\"memory bandwidth\"}],[\"$\",\"$L10\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/model%20optimization\",\"children\":\"model optimization\"}],[\"$\",\"$L10\",\"13\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/mixture%20of%20experts\",\"children\":\"mixture of experts\"}],[\"$\",\"$L10\",\"14\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/moe\",\"children\":\"moe\"}],[\"$\",\"$L10\",\"15\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/quantization\",\"children\":\"quantization\"}],[\"$\",\"$L10\",\"16\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/speculative%20decoding\",\"children\":\"speculative decoding\"}],[\"$\",\"$L10\",\"17\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/kv%20cache\",\"children\":\"kv cache\"}],[\"$\",\"$L10\",\"18\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/economics\",\"children\":\"economics\"}],[\"$\",\"$L10\",\"19\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/infrastructure\",\"children\":\"infrastructure\"}],[\"$\",\"$L10\",\"20\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/hacker%20news\",\"children\":\"hacker news\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Batching \u0026 Parallelism\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Large-scale inference relies on batching, where multiple user requests are processed simultaneously.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"The primary bottleneck in LLM inference is memory bandwidth (loading model weights from VRAM to compute units).\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"By batching requests, weights are loaded once and applied to many queries, amortizing the high cost of memory access and dramatically increasing throughput.\"}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":\"This process is highly parallelizable across large clusters of GPUs.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Hardware \u0026 Infrastructure\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Massive investment in specialized hardware like NVIDIA H100s or Google TPUs, which feature large amounts of high-bandwidth memory (HBM).\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Models are sharded (split) across multiple GPUs (tensor and pipeline parallelism), so a single model runs on a cluster of interconnected accelerators.\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"The infrastructure is built for high utilization, unlike a local setup which is mostly idle.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"15:[\"$\",\"div\",\"2\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$L10\",null,{\"href\":\"https://world.hey.com/dhh/the-framework-desktop-is-a-beast-636fb4ff\",\"target\":\"_blank\",\"children\":\"The Framework Desktop is a beast\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$L10\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/hardware\",\"children\":\"hardware\"}],[\"$\",\"$L10\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/desktop%20computer\",\"children\":\"desktop computer\"}],[\"$\",\"$L10\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/mini%20pc\",\"children\":\"mini pc\"}],[\"$\",\"$L10\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/framework\",\"children\":\"framework\"}],[\"$\",\"$L10\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/framework%20desktop\",\"children\":\"framework desktop\"}],[\"$\",\"$L10\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/amd\",\"children\":\"amd\"}],[\"$\",\"$L10\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ryzen\",\"children\":\"ryzen\"}],[\"$\",\"$L10\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/apple\",\"children\":\"apple\"}],[\"$\",\"$L10\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/m4\",\"children\":\"m4\"}],[\"$\",\"$L10\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/mac%20studio\",\"children\":\"mac studio\"}],[\"$\",\"$L10\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/intel\",\"children\":\"intel\"}],[\"$\",\"$L10\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/nvidia\",\"children\":\"nvidia\"}],[\"$\",\"$L10\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/rtx%204060\",\"children\":\"rtx 4060\"}],[\"$\",\"$L10\",\"13\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/cpu\",\"children\":\"cpu\"}],[\"$\",\"$L10\",\"14\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpu\",\"children\":\"gpu\"}],[\"$\",\"$L10\",\"15\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/unified%20memory\",\"children\":\"unified memory\"}],[\"$\",\"$L10\",\"16\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/small%20form%20factor\",\"children\":\"small form factor\"}],[\"$\",\"$L10\",\"17\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/sff\",\"children\":\"sff\"}],[\"$\",\"$L10\",\"18\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/performance\",\"children\":\"performance\"}],[\"$\",\"$L10\",\"19\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/benchmark\",\"children\":\"benchmark\"}],[\"$\",\"$L10\",\"20\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/geekbench\",\"children\":\"geekbench\"}],[\"$\",\"$L10\",\"21\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/development\",\"children\":\"development\"}],[\"$\",\"$L10\",\"22\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/docker\",\"children\":\"docker\"}],[\"$\",\"$L10\",\"23\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gaming\",\"children\":\"gaming\"}],[\"$\",\"$L10\",\"24\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ai\",\"children\":\"ai\"}],[\"$\",\"$L10\",\"25\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/large%20language%20models\",\"children\":\"large language models\"}],[\"$\",\"$L10\",\"26\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/llm\",\"children\":\"llm\"}],[\"$\",\"$L10\",\"27\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/review\",\"children\":\"review\"}],[\"$\",\"$L10\",\"28\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/value\",\"children\":\"value\"}],[\"$\",\"$L10\",\"29\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/pricing\",\"children\":\"pricing\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"CPU:\"}],\" AMD Ryzen AI Max 395+ (16 Zen5 cores @ 5.1GHz), a laptop-class processor.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Form Factor:\"}],\" 4.5L volume, noted for being quiet even under full load.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Multi-Core Performance:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Outperforms Apple M4 Max, M4 Pro, and Intel 14900K in Geekbench 6 multi-core benchmarks.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Excels in Docker-based development workflows (e.g., Ruby test suite with MySQL/Redis/ElasticSearch), showing a ~40% speed increase over an M4 Max, partially due to Docker's native performance on Linux.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"div\",\"3\",{\"className\":\"ArticlePreview_ArticlePreview__59E_4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"ArticlePreview_title__Snpua\",\"children\":[\"$\",\"$L10\",null,{\"href\":\"https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the\",\"target\":\"_blank\",\"children\":\"From GPT-2 to gpt-oss: Analyzing the Architectural Advances\"}]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_tags__y8wnE\",\"children\":[[\"$\",\"$L10\",\"0\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/large%20language%20model\",\"children\":\"large language model\"}],[\"$\",\"$L10\",\"1\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/llm\",\"children\":\"llm\"}],[\"$\",\"$L10\",\"2\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/model%20architecture\",\"children\":\"model architecture\"}],[\"$\",\"$L10\",\"3\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpt-oss\",\"children\":\"gpt-oss\"}],[\"$\",\"$L10\",\"4\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpt-2\",\"children\":\"gpt-2\"}],[\"$\",\"$L10\",\"5\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/mixture-of-experts\",\"children\":\"mixture-of-experts\"}],[\"$\",\"$L10\",\"6\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/moe\",\"children\":\"moe\"}],[\"$\",\"$L10\",\"7\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/rmsnorm\",\"children\":\"rmsnorm\"}],[\"$\",\"$L10\",\"8\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/swiglu\",\"children\":\"swiglu\"}],[\"$\",\"$L10\",\"9\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/rotary%20position%20embeddings\",\"children\":\"rotary position embeddings\"}],[\"$\",\"$L10\",\"10\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/rope\",\"children\":\"rope\"}],[\"$\",\"$L10\",\"11\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/grouped%20query%20attention\",\"children\":\"grouped query attention\"}],[\"$\",\"$L10\",\"12\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gqa\",\"children\":\"gqa\"}],[\"$\",\"$L10\",\"13\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/sliding-window%20attention\",\"children\":\"sliding-window attention\"}],[\"$\",\"$L10\",\"14\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/quantization\",\"children\":\"quantization\"}],[\"$\",\"$L10\",\"15\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/reinforcement%20learning\",\"children\":\"reinforcement learning\"}],[\"$\",\"$L10\",\"16\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/open%20source\",\"children\":\"open source\"}],[\"$\",\"$L10\",\"17\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/apache%202.0\",\"children\":\"apache 2.0\"}],[\"$\",\"$L10\",\"18\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/ai\",\"children\":\"ai\"}],[\"$\",\"$L10\",\"19\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/deep%20learning\",\"children\":\"deep learning\"}],[\"$\",\"$L10\",\"20\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/technical%20analysis\",\"children\":\"technical analysis\"}],[\"$\",\"$L10\",\"21\",{\"className\":\"ArticlePreview_tag___oIyn\",\"href\":\"/tag/gpu\",\"children\":\"gpu\"}]]}],[\"$\",\"div\",null,{\"className\":\"ArticlePreview_summary__Zyb4E\",\"children\":[[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[\"$\",\"strong\",\"strong-0\",{\"children\":\"Architectural Evolution from GPT-2:\"}]}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Normalization:\"}],\" Replaced LayerNorm with the computationally simpler RMSNorm.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Activation:\"}],\" Switched from GELU to a gated SwiGLU (Swish Gated Linear Unit), offering better expressivity with fewer parameters.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Positional Encoding:\"}],\" Upgraded from learned absolute positional embeddings to Rotary Position Embeddings (RoPE).\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Attention:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Uses Grouped Query Attention (GQA) for improved efficiency over Multi-Head Attention (MHA).\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Implements sliding-window attention (128-token window) in every other layer, a technique reportedly used in GPT-3.\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":\"Re-introduces bias units in attention projections and adds learned \\\"attention sinks\\\" (per-head bias logits) to stabilize attention over long contexts.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",\"$L1d\",\"\\n\"]}],\"\\n\"]}],\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\"]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"li\",\"li-2\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Model Optimizations\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Mixture of Experts (MoE)\"}],\": Models are designed so that only a fraction of the total parameters (the \\\"experts\\\") are activated for any given token, reducing the computational cost per inference.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Quantization\"}],\": Model weights are compressed to lower-precision formats (e.g., INT8, FP8) to reduce memory footprint and bandwidth requirements.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Speculative Decoding\"}],\": A smaller, faster \\\"draft\\\" model generates candidate tokens which are then validated in parallel by the larger, more powerful model, increasing token generation speed.\"]}],\"\\n\",[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"KV Cache\"}],\": The key-value state of the attention mechanism is cached, so the context doesn't need to be re-processed for each new token generated in a sequence.\"]}],\"\\n\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"li\",\"li-3\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Economic Model\"}],\":\"]}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"The operation is funded by billions of dollars in investment, allowing companies to run at a loss to capture market share.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"The cost per user is manageable because of the efficiencies of scale and because the vast majority of users are idle most of the time, allowing for high multi-tenancy on the hardware.\"}],\"\\n\"]}],\"\\n\"]}]\n19:[\"$\",\"li\",\"li-3\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Single-Core Performance:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Approximately 20% slower than Apple's M4 series in single-core tasks.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Speedometer 2.1 benchmark: 670 (vs. 744 on M4 Pro).\"}],\"\\n\"]}],\"\\n\"]}]\n1a:[\"$\",\"li\",\"li-4\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Memory:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Utilizes unified memory, configurable up to 128GB.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Suitable for running large local LLMs (e.g., 120b models at ~40 tokens/second).\"}],\"\\n\"]}],\"\\n\"]}]\n1b:[\"$\",\"li\",\"li-5\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Graphics:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"Integrated GPU performance is comparable to a discrete NVIDIA RTX 4060.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Capable of running modern games at 1440p on high settings.\"}],\"\\n\"]}],\"\\n\"]}]\n1c:[\"$\",\"li\",\"li-6\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Value:\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"A 64GB RAM / 2TB NVMe configuration is priced at $1,876, nearly half the cost of a similarly specced Mac Studio ($3,299).\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"Positioned as a higher-performance, more expensive alternative to other mini PCs like the Beelink SER9.\"}],\"\\n\"]}],\"\\n\"]}]\n1d:[\"$\",\"li\",\"li-4\",{"])</script><script>self.__next_f.push([1,"\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Regularization:\"}],\" Removed dropout, which is common in modern LLMs trained on massive datasets for a single epoch.\"]}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"li\",\"li-1\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[\"$\",\"strong\",\"strong-0\",{\"children\":\"Core Architecture:\"}]}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Mixture-of-Experts (MoE):\"}],\" Replaces standard feed-forward networks with MoE layers. The \",[\"$\",\"code\",\"code-0\",{\"children\":\"gpt-oss-20b\"}],\" model uses 32 experts and activates 4 per token.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Width vs. Depth:\"}],\" Compared to the contemporary Qwen3 model, \",[\"$\",\"code\",\"code-0\",{\"children\":\"gpt-oss\"}],\" is a \\\"wider\\\" architecture (larger embedding dimension) rather than \\\"deeper\\\" (fewer transformer blocks).\"]}],\"\\n\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"1f:[\"$\",\"li\",\"li-2\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[\"$\",\"strong\",\"strong-0\",{\"children\":\"Training and Features:\"}]}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Reasoning Control:\"}],\" Trained with a high-compute reinforcement learning stage that allows users to control reasoning verbosity and accuracy at inference time via a system prompt (\",[\"$\",\"code\",\"code-0\",{\"children\":\"Reasoning effort: low/medium/high\"}],\").\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Quantization:\"}],\" Released with an MXFP4 quantization scheme for MoE experts, enabling the 120B model to run on a single 80GB H100 GPU and the 20B model on a 16GB RTX 50-series GPU.\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"License:\"}],\" Apache 2.0 open-weight license.\"]}],\"\\n\"]}],\"\\n\"]}]\n"])</script></body></html>